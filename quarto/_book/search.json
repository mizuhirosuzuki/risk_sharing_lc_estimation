[
  {
    "objectID": "laczo2015_dataprep.html#load-packages",
    "href": "laczo2015_dataprep.html#load-packages",
    "title": "1  Data preparation",
    "section": "1.1 Load packages",
    "text": "1.1 Load packages\n\npacman::p_load(\n  tidyverse,\n  kableExtra\n)"
  },
  {
    "objectID": "laczo2015_dataprep.html#load-datasets",
    "href": "laczo2015_dataprep.html#load-datasets",
    "title": "1  Data preparation",
    "section": "1.3 Load datasets",
    "text": "1.3 Load datasets\n\nload('Laczo2015/allest')\n\n\n1.3.1 Basic data processes of raw data\n\nnumVillages <- max(villagedat)\nvillageIndicatorMatrix <- do.call(\n  cbind,\n  map(\n    seq(1, numVillages),\n    ~ villagedat[, 1] == .\n  )\n)\n\n\ncreateVillageAggregateByYear <- function(\n    data, \n    func, \n    numVillages,\n    villageIndicatorMatrix\n    ) {\n  do.call(\n    rbind, \n    map(\n      seq(1, numVillages),\n      ~ data[villageIndicatorMatrix[, .],] %>% func\n      )\n    )\n  }\n\ncreateVillageAggregate <- function(\n    data, \n    func, \n    numVillages,\n    villageIndicatorMatrix\n    ) {\n    map_vec(\n      seq(1, numVillages),\n      ~ data[villageIndicatorMatrix[, .],] %>% func\n    )\n  }\n\n\nvilMeanIncByYear <- createVillageAggregateByYear(incdat, colMeans, numVillages, villageIndicatorMatrix)\nvilMeanConsByYear <- createVillageAggregateByYear(consdat, colMeans, numVillages, villageIndicatorMatrix)\nvilMeanLogCons <- createVillageAggregateByYear(log(consdat), colMeans, numVillages, villageIndicatorMatrix)\n\n\nincLow <- createVillageAggregate(\n  incdat, \n  function(x) quantile(x, 0.025, na.rm = TRUE), \n  numVillages, \n  villageIndicatorMatrix\n  )\nincHigh <- createVillageAggregate(\n  incdat, \n  function(x) quantile(x, 0.975, na.rm = TRUE),\n  numVillages, \n  villageIndicatorMatrix\n  )\n\nSince there is no saving in the model, the consumption and income should coincide. For this, I rescale the income so that the means of consumption and income are identical in each village.\n\nvilConsPerIncByYear <- vilMeanConsByYear / vilMeanIncByYear\nincdatRescaled <- incdat * (villageIndicatorMatrix %*% vilConsPerIncByYear)\nvilMeanIncByYearRescaled <- createVillageAggregateByYear(\n  incdatRescaled, colMeans, numVillages, villageIndicatorMatrix\n  )\n\n\nincRescaledLow <- createVillageAggregate(\n  incdatRescaled, \n  function(x) quantile(x, 0.025, na.rm = TRUE), \n  numVillages, \n  villageIndicatorMatrix\n  )\nincRescaledHigh <- createVillageAggregate(\n  incdatRescaled, \n  function(x) quantile(x, 0.975, na.rm = TRUE),\n  numVillages, \n  villageIndicatorMatrix\n  )\n\nincdatRescaledWinsorized <- incdatRescaled %>% \n  pmax(as.vector(villageIndicatorMatrix %*% incRescaledLow)) %>% \n  pmin(as.vector(villageIndicatorMatrix %*% incRescaledHigh))"
  },
  {
    "objectID": "laczo2015_dataprep.html#estimate-income-processes",
    "href": "laczo2015_dataprep.html#estimate-income-processes",
    "title": "1  Data preparation",
    "section": "1.4 Estimate income processes",
    "text": "1.4 Estimate income processes\nThe main step of this data processing is to estimate the income process, first of households and second of the “village” (= average of households in a village). We want the income process of the village since, in estimation, we consider the risk-sharing transfers between a household and the village so that we can consider one-to-one transfers instead of one-to-n transfers.\nI follow the estimation process of Laczó (2015), which is detailed in the paper’s online appendix First I estimate the income process of households by types, where the types are by income’s mean and coefficient of variance (CV). Here, in total we have 4 types (high/low mean income X high/low CV income). If there were long enough income data, we would be able to estimate the income process of each household, but given the 6 observations per household for income in ICRISAT data, this is not feasible. Also, to mitigate the effect of outliers, income smaller than 2.5 percentile or larger than 97.5 percentile in each village is not used for estimation.\n\nnumIncomeStatesHH <- 8\nnumIncomeStatesVillage <- 5\nnumIncomeStates <- numIncomeStatesHH * numIncomeStatesVillage\n\n\n1.4.1 Household\n\nhouseholdIncMean <- incdatRescaled %>% rowMeans\nhouseholdIncSD <- apply(incdatRescaled, 1, sd, na.rm = TRUE)\nhouseholdIncCV <- householdIncSD / householdIncMean\n\nvilIncMeanMedian <- createVillageAggregate(\n  as.matrix(householdIncMean), \n  median,\n  numVillages,\n  villageIndicatorMatrix\n  )\nvilIncCVMedian <- createVillageAggregate(\n  as.matrix(householdIncCV), \n  median,\n  numVillages,\n  villageIndicatorMatrix\n  )\n\n\n1.4.1.1 Estimate AR(1) process\nFirst I estimate the AR(1) process of households’ income. In particular, the following AR(1) process is estimated:\n\n  y_{it} = (1 - \\rho) \\mu + \\rho y_{i, t - 1} + u_{it},\n\nwhere y_{it} is the income of a household i in period t.\nThe parameters are given as\n\n  \\mu = E(y_{it}) \\\\\n  \\rho = Cor(y_{it}, y_{i, t - 1}) \\\\\n  \\sigma_{u}^2 = (1 - \\rho^2) Var(y_{it}).\n\n\nhouseholdIncMeanClassVec <- (\n  householdIncMean > (villageIndicatorMatrix %*% vilIncMeanMedian)\n  ) + 1\nhouseholdIncCVClassVec <- (\n  householdIncCV > (villageIndicatorMatrix %*% vilIncCVMedian)\n  ) + 1\n\nincdatRescaledForAR1Estimation <- incdatRescaled\nlaggedIncdatRescaledForAR1Estimation <- cbind(\n  NA, \n  incdatRescaled[, seq(1, tnum - 1)]\n  )\n\nincdatRescaledForAR1Estimation[\n  (incdat <= as.vector(villageIndicatorMatrix %*% incLow)) |\n    (incdat >= as.vector(villageIndicatorMatrix %*% incHigh))] <- NA\nlaggedIncdatRescaledForAR1Estimation[\n  (incdat <= as.vector(villageIndicatorMatrix %*% incLow)) |\n    (incdat >= as.vector(villageIndicatorMatrix %*% incHigh))] <- NA\n\ngetDataByMeanCVClassByVillage <- function(\n    village, \n    data, \n    meanClass, \n    CVClass,\n    meanClassVec,\n    CVClassVec,\n    villageIndicatorMatrix\n    ) {\n  data[\n    (meanClassVec == meanClass) & \n      (CVClassVec == CVClass) &\n      (villageIndicatorMatrix[, village])\n  ]\n}\n\ncalculateAR1Parameters <- function(data, laggedData) {\n  mu <- mean(data, na.rm = TRUE)\n  rho <- cor(\n    data,\n    laggedData,\n    use = \"complete.obs\"\n    )\n  sigmau <- sqrt(var(data, na.rm = TRUE) * (1 - rho^2))\n  \n  return(list(mu = mu, rho = rho, sigmau = sigmau))\n}\n\n\n\n1.4.1.2 Approximate the AR(1) process with Markov chain for income of households\nGiven the estimated parameters, I approximate the AR(1) process with discretization. For this, Laczó (2015) used Tauchen’s method with a small modification that, instead of assigning the bounds of grid points with a parameter, the quantiles of income distributions are used to determine each grid point. To guarantee that the mean income at the steady state of the estimated process coincides with the actual mean income, the grid points are rescaled at the end.\n\ncalculateGridPoints <- function(numStates, data) {\n  gridQuantile <- seq(0, 1, by = 1 / numStates)\n  map_dbl(\n    (gridQuantile[1:(length(gridQuantile) - 1)] + gridQuantile[2:length(gridQuantile)]) / 2,\n    ~ quantile(data, ., na.rm = TRUE)\n  )\n}\n\napproximateAR1Tauchen <- function(numStates, data, mu, rho, sigma) {\n  \n  gridPoints <- calculateGridPoints(numStates, data)\n  \n  transitionMatrix <- array(NA, c(numStates, numStates))\n  \n  for (currentState in 1:numStates) {\n    transitionMatrix[currentState, 1] <- (\n      pnorm(\n        ((gridPoints[2] + gridPoints[1]) / 2 \n         - (1 - rho) * mu - rho * gridPoints[currentState])\n        / sigma\n        )\n      )\n    transitionMatrix[currentState, numStates] <- 1 - pnorm(\n      ((gridPoints[numStates] + gridPoints[numStates - 1]) / 2 \n       - (1 - rho) * mu - rho * gridPoints[currentState])\n      / sigma\n      )\n  }\n  \n  for (currentState in 1:numStates) {\n    for (nextState in 2:(numStates - 1)) {\n        transitionMatrix[currentState, nextState] <- (\n          pnorm(\n          ((gridPoints[nextState + 1] + gridPoints[nextState]) / 2 \n           - (1 - rho) * mu - rho * gridPoints[currentState])\n          / sigma\n          )\n        - pnorm(\n          ((gridPoints[nextState] + gridPoints[nextState - 1]) / 2 \n           - (1 - rho) * mu - rho * gridPoints[currentState])\n          / sigma\n          )\n          )\n        }\n  }\n  \n  return(list(transitionMatrix = transitionMatrix, gridPoints = gridPoints))\n}\n\ncalculateSteadyStateProb <- function(transitionMatrix) {\n  (\n    eigen(t(transitionMatrix))$vector[, 1]\n    / sum(eigen(t(transitionMatrix))$vector[, 1])\n  )\n}\n\nrescaleGridPoints <- function(transitionMatrix, gridPoints, data) {\n  steadyStateProb <- calculateSteadyStateProb(transitionMatrix)\n  rescaleScalar <- as.numeric(\n    mean(data, na.rm = TRUE) / gridPoints \n    %*% steadyStateProb\n    )\n  gridPointsRescaled <- gridPoints * rescaleScalar\n  return(list(\n    gridPointsRescaled = gridPointsRescaled,\n    steadyStateProb = steadyStateProb\n    ))\n}\n\napproximateAR1TauchenWithRescaling <- function(numStates, data, mu, rho, sigma) {\n  \n  TauchenResult <- approximateAR1Tauchen(numStates, data, mu, rho, sigma)\n  transitionMatrix <- TauchenResult$transitionMatrix\n  gridPoints <- TauchenResult$gridPoints\n  gridPointsRescaledResult <- rescaleGridPoints(\n    transitionMatrix, gridPoints, data\n    )\n  gridPointsRescaled <- gridPointsRescaledResult$gridPointsRescaled\n  steadyStateProb <- gridPointsRescaledResult$steadyStateProb\n  \n  return(list(\n    transitionMatrix = transitionMatrix, \n    gridPointsRescaled = gridPointsRescaled,\n    steadyStateProb = steadyStateProb\n    ))\n}\n\nestimateHouseholdIncomeTransitionProcessByVillage <- function(\n    village,\n    numStates,\n    data,\n    laggedData,\n    householdIncMeanClassVec,\n    householdIncCVClassVec,\n    villageIndicatorMatrix\n) {\n  gridPointsArray <- array(NA, c(2, 2, numStates))\n  transitionMatrixArray <- array(NA, c(2, 2, numStates, numStates))\n  steadyStateProbArray <- array(NA, c(2, 2, numStates)) \n  AR1ParametersArray <- array(NA, c(2, 2, 3))\n  \n  for (incomeMeanClass in seq(1, 2)) {\n    for (incomeCVClass in seq(1, 2)) {\n      incdatRescaledMeanCVClass <- getDataByMeanCVClassByVillage(\n        village, data, incomeMeanClass, incomeCVClass, \n        householdIncMeanClassVec, householdIncCVClassVec, villageIndicatorMatrix\n        )\n      laggeedIncdatRescaledMeanCVClass <- getDataByMeanCVClassByVillage(\n        village, laggedData, incomeMeanClass, incomeCVClass,\n        householdIncMeanClassVec, householdIncCVClassVec, villageIndicatorMatrix\n        )\n      AR1Parameters <- calculateAR1Parameters(incdatRescaledMeanCVClass, laggeedIncdatRescaledMeanCVClass)\n      TauchenResult <- approximateAR1TauchenWithRescaling(\n        numIncomeStatesHH, incdatRescaledMeanCVClass, \n        AR1Parameters$mu, AR1Parameters$rho, AR1Parameters$sigmau\n        )\n      gridPointsArray[incomeMeanClass, incomeCVClass,] <- TauchenResult$gridPoints\n      transitionMatrixArray[incomeMeanClass, incomeCVClass,,] <- TauchenResult$transitionMatrix\n      steadyStateProbArray[incomeMeanClass, incomeCVClass,] <- TauchenResult$steadyStateProb\n      AR1ParametersArray[incomeMeanClass, incomeCVClass,] <- unlist(AR1Parameters)\n    }\n  }\n  \n  return(list(\n    gridPointsArray = gridPointsArray,\n    transitionMatrixArray = transitionMatrixArray,\n    steadyStateProbArray = steadyStateProbArray,\n    AR1ParametersArray = AR1ParametersArray,\n    numHouseholds = villageIndicatorMatrix[, village] %>% sum\n    ))\n}\n  \nhouseholdAR1EstimationResult <- map(\n  seq(1, numVillages),\n  ~ estimateHouseholdIncomeTransitionProcessByVillage(\n    ., \n    numIncomeStatesHH, \n    incdatRescaledForAR1Estimation, \n    laggedIncdatRescaledForAR1Estimation,\n    householdIncMeanClassVec,\n    householdIncCVClassVec,\n    villageIndicatorMatrix\n    )\n)\n\n\n\n\n\n\n\n1.4.2 Village\nGiven the estimated household income processes, I estimate the income process of the “village”. For this, I first simulate the average village income, using the parameters estimated above. In particular, I simulate the household income over 1000 periods, then compute the mean income in each period. After excluding the first 100 observations to use the income at the steady state, I estimate the parameters using the same method as for the household income.\n\n1.4.2.1 Simulate income process for village\n\nnumVillageIncomeSimulations <- 1000\nnumVillageIncomeSimulationsPeriodDrop <- 100\n\n\nsample1stPeriodIncomeStateByMeanCVClass <- function(\n    meanClass, \n    CVClass, \n    numStates, \n    steadyStateProbArray\n    ) {\n  sample(\n    seq(1, numStates),\n    1,\n    prob = steadyStateProbArray[meanClass, CVClass, ]\n    )\n}\n\nsampleConditionalIncomeStateByMeanCVClass <- function(\n    meanClass, \n    CVClass,\n    numStates,\n    transitionMatrixArray,\n    previousState\n) {\n  sample(\n    seq(1, numStates),\n    1,\n    prob = transitionMatrixArray[meanClass, CVClass, previousState,]\n  )\n}\n\nsampleAllIncomeStateByMeanCVClass <- function(\n    meanClass, \n    CVClass,\n    numStates,\n    steadyStateProbArray,\n    transitionMatrixArray,\n    numSimulations = numVillageIncomeSimulations\n) {\n  incomeStateVector <- vector(mode = \"integer\", length = numSimulations)\n  incomeStateVector[1] <- sample1stPeriodIncomeStateByMeanCVClass(\n    meanClass, \n    CVClass, \n    numStates, \n    steadyStateProbArray\n    )\n  for (period in seq(2, numSimulations)) {\n    incomeStateVector[period] <- sampleConditionalIncomeStateByMeanCVClass(\n      meanClass, \n      CVClass,\n      numStates,\n      transitionMatrixArray,\n      incomeStateVector[period - 1]\n      )\n  }\n  return(incomeStateVector)\n}\n\nsimulateHouseholdIncomeByMeanCVClass <- function(\n    meanClass, \n    CVClass,\n    numStates,\n    steadyStateProbArray,\n    transitionMatrixArray,\n    gridPointsArray,\n    numSimulations = numVillageIncomeSimulations\n) {\n  incomeStateVec <- sampleAllIncomeStateByMeanCVClass(\n    meanClass, \n    CVClass,\n    numStates,\n    steadyStateProbArray,\n    transitionMatrixArray)\n  gridPointsArray[meanClass, CVClass, incomeStateVec]\n}\n\nsimulateHouseholdIncomeByVillage <- function(\n    village,\n    meanClassVec,\n    CVClassVec,\n    numStates,\n    householdAR1EstimationResult,\n    villageIndicatorMatrix\n) {\n  \n  meanClassVillage <- meanClassVec[villageIndicatorMatrix[, village]]\n  CVClassVillage <- CVClassVec[villageIndicatorMatrix[, village]]\n  steadyStateProbArrayVillage <- householdAR1EstimationResult[[village]]$steadyStateProbArray\n  transitionMatrixArrayVillage <- householdAR1EstimationResult[[village]]$transitionMatrixArray\n  gridPointsArrayVillage <- householdAR1EstimationResult[[village]]$gridPointsArray\n  \n  do.call(\n    rbind,\n    map2(\n      meanClassVillage, CVClassVillage,\n      ~ simulateHouseholdIncomeByMeanCVClass(\n        .x, .y, \n        numStates, \n        steadyStateProbArrayVillage, \n        transitionMatrixArrayVillage,\n        gridPointsArrayVillage\n        )\n    )\n  )\n  \n}\n\n\n\n1.4.2.2 Estimate the income process of the village\nGiven the simulated income data, I estimate the income process parameters and approximate the process with Tauchen’s method.\n\nestimateVillagencomeTransitionProcessByVillage <- function(\n    village,\n    meanClassVec,\n    CVClassVec,\n    numStatesHH,\n    numStatesVillage,\n    householdAR1EstimationResult,\n    villageIndicatorMatrix,\n    numVillageIncomeSimulationsPeriodDrop,\n    numVillageIncomeSimulations\n){\n  householdIncomeSimulationResult <- simulateHouseholdIncomeByVillage(\n    village,\n    meanClassVec,\n    CVClassVec,\n    numStatesHH,\n    householdAR1EstimationResult,\n    villageIndicatorMatrix\n  )\n  \n  villageSimulatedIncMean <- colMeans(\n    householdIncomeSimulationResult[\n      , numVillageIncomeSimulationsPeriodDrop:numVillageIncomeSimulations\n      ]\n  )\n  villageSimulatedLaggedIncMean <- colMeans(\n    householdIncomeSimulationResult[\n      , (numVillageIncomeSimulationsPeriodDrop - 1):(numVillageIncomeSimulations - 1)\n      ]\n  )\n  \n  villageAR1Parameters <- calculateAR1Parameters(\n    villageSimulatedIncMean,\n    villageSimulatedLaggedIncMean \n  )\n  \n  villageAR1TauchenApproximation <- approximateAR1Tauchen(\n    numStatesVillage, villageSimulatedIncMean, \n    villageAR1Parameters$mu, villageAR1Parameters$rho, villageAR1Parameters$sigmau\n  )\n  \n  villageIncomeGridPoints <- calculateGridPoints(numStatesVillage, villageSimulatedIncMean)\n  \n  villageIncomeGridPointsRescaled <- rescaleGridPoints(\n      villageAR1TauchenApproximation$transitionMatrix, \n      villageIncomeGridPoints,\n      villageSimulatedIncMean\n      )\n  \n  return(list(\n    transitionMatrix = villageAR1TauchenApproximation$transitionMatrix,\n    gridPoints = villageIncomeGridPointsRescaled$gridPointsRescaled\n  ))\n}\n\nvillageAR1EstimationResult <- map(\n  seq(1, numVillages),\n  ~ estimateVillagencomeTransitionProcessByVillage(\n      .,\n      householdIncMeanClassVec,\n      householdIncCVClassVec,\n      numIncomeStatesHH,\n      numIncomeStatesVillage,\n      householdAR1EstimationResult,\n      villageIndicatorMatrix,\n      numVillageIncomeSimulationsPeriodDrop,\n      numVillageIncomeSimulations\n  )\n)"
  },
  {
    "objectID": "laczo2015_dataprep.html#sanity-check-compare-against-the-parameters-in-the-original-paper",
    "href": "laczo2015_dataprep.html#sanity-check-compare-against-the-parameters-in-the-original-paper",
    "title": "1  Data preparation",
    "section": "1.5 Sanity check: compare against the parameters in the original paper",
    "text": "1.5 Sanity check: compare against the parameters in the original paper\nJust for a sanity check of my code, I compare the household AR(1) process parameters I derived against the ones provided in the appendix of the paper. The parameters in the tables below coincide to the parameters in the paper appendix, excpet the ones for “Low mean, high risk” in Shirapur. I reran the original R script and confirmed that the correct parameters are the ones that I am showing in the table below.\nSince I refactored the origianl R script, I cannot reproduce the village parameters. To be clear, the author has set the random seed in the code, and the reason I cannot reproduce the village parameters is merely because of the difference in code structures.\n\ncreateParameterTableByVillage <- function(\n    village,\n    householdAR1EstimationResult\n) {\n  \n  villageParameters <- array(NA, c(3, 4))\n  colIndex <- 0\n  for (incomeMeanClass in seq(1, 2)) {\n    for (incomeCVClass in seq(1, 2)) {\n      colIndex <- colIndex + 1\n      villageParameters[, colIndex] <- \n        householdAR1EstimationResult[[village]]$AR1ParametersArray[incomeMeanClass, incomeCVClass,]\n    }\n  }\n  rownames(villageParameters) <- c(\n    \"mu\",\n    \"rho\",\n    \"sigmau_squared\"\n  )\n  villageParameters %>% \n    kbl(digits = 3) %>% \n    kable_classic() %>% \n    add_header_above(\n      c(\n        \"\",\n        \"Low mean, \\n low risk\",\n        \"Low mean, \\n high risk\",\n        \"High mean, \\n low risk\",\n        \"High mean, \\n high risk\"\n      )\n    )\n}\ncreateParameterTableByVillage(1, householdAR1EstimationResult)\n\n\n\n\n\nLow mean,  low risk\nLow mean,  high risk\nHigh mean,  low risk\nHigh mean,  high risk\n\n\n  \n    mu \n    182.475 \n    160.201 \n    441.843 \n    391.798 \n  \n  \n    rho \n    0.189 \n    0.626 \n    0.550 \n    0.365 \n  \n  \n    sigmau_squared \n    49.088 \n    68.506 \n    128.741 \n    188.458 \n  \n\n\n\n\ncreateParameterTableByVillage(2, householdAR1EstimationResult)\n\n\n\n\n\nLow mean,  low risk\nLow mean,  high risk\nHigh mean,  low risk\nHigh mean,  high risk\n\n\n  \n    mu \n    235.906 \n    243.872 \n    506.555 \n    525.520 \n  \n  \n    rho \n    0.491 \n    0.285 \n    0.846 \n    0.520 \n  \n  \n    sigmau_squared \n    45.808 \n    79.111 \n    122.302 \n    237.918 \n  \n\n\n\n\ncreateParameterTableByVillage(3, householdAR1EstimationResult)\n\n\n\n\n\nLow mean,  low risk\nLow mean,  high risk\nHigh mean,  low risk\nHigh mean,  high risk\n\n\n  \n    mu \n    263.292 \n    288.625 \n    446.590 \n    642.628 \n  \n  \n    rho \n    0.318 \n    -0.150 \n    0.160 \n    0.199 \n  \n  \n    sigmau_squared \n    78.793 \n    126.987 \n    129.785 \n    271.997"
  },
  {
    "objectID": "laczo2015_dataprep.html",
    "href": "laczo2015_dataprep.html",
    "title": "1  My document",
    "section": "",
    "text": "2 Laczo (2015) R scripts for data preparation"
  },
  {
    "objectID": "laczo2015_dataprep.html#global-settings",
    "href": "laczo2015_dataprep.html#global-settings",
    "title": "1  Data preparation",
    "section": "1.2 Global settings",
    "text": "1.2 Global settings\n\nset.seed(123)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estimation of risk-sharing model with limited commitment",
    "section": "",
    "text": "Preface\nIn this page, I extensively use the codes by Laczó (2015), which can be downloaded here. Since I am not using her codes directly but rather I used her codes to understand the computational process, our codes look very different and all errors and bugs in the code below are on my own.\n\n\n\n\nLaczó, Sarolta. 2015. “Risk Sharing with Limited Commitment and Preference Heterogeneity: Structural Estimation and Testing.” Journal of the European Economic Association 13 (2): 265–92."
  },
  {
    "objectID": "laczo2015_full_hom.html",
    "href": "laczo2015_full_hom.html",
    "title": "2  Full risk-sharing under homogeneous preferences",
    "section": "",
    "text": "3 Full risk-sharing with homogeneous preferences"
  },
  {
    "objectID": "laczo2015_full_hom.html#theory",
    "href": "laczo2015_full_hom.html#theory",
    "title": "2  Full risk-sharing under homogeneous preferences",
    "section": "2.1 Theory",
    "text": "2.1 Theory\nTo derive the likelihood function, I review the theoretical result.\n\n2.1.1 Settings\nSuuposed that there are N households in a village. Each household maximized its expected lifetime utility: \n  E \\left[ \\sum_{t = 1}^{\\infty} \\delta^t u (c_{it}) \\right],\n where \\delta is the discount factor, u is instantaneous preference of households (note that here I assume homogeneous preference), and c_{it} is consumption by household i at time t. Assume that there is no saving.\nFor income, let s^t = (s_1, \\dots, s_t) the history of income states from time 1 to t, and let y_{it}(s_t) be household i’s income at time t and state s_t. Assume that the income process is a Markov process and is independent across households.\nWith these, I find the Pareto-optimal allocations. For this, a weighted sum of households’ expected lifetime utilities is maximized. In other words, a social planner solves the following maximization problem:\n\n  \\max_{\\{c_{it}(s^t)\\}} \\quad \\sum_i \\lambda_i \\sum_{t = 1}^{\\infty} \\sum_{s^t} \\delta^t \\pi(s^t) u(c_{it}(s^t))\n subject to the resource constraint \n  \\sum_i c_{it}(s^t) \\le \\sum_i y_{it}(s_t) \\quad \\forall s^t, \\forall t,\n where \\lambda_i is the Pareto weight of i, \\pi(s^t) is the probability that the history s^t is realized, and c_{it}(s^t) is i’s consumption under the history s^t.\n\n\n2.1.2 Optimization condition\nSolving this maximization problem, we can get a well-known result that, for any two households i and j in the village, the following holds: \n  \\frac{u'(c_{jt}(s^t))}{u'(c_{it}(s^t))} = \\frac{\\lambda_i}{\\lambda_j} \\quad \\forall s^t, \\forall t.\n\nIf I assume the utility function to take the CRRA form, that is, \n  u(c_{it}) = \\frac{c_{it}^{1 - \\sigma} - 1}{1 - \\sigma},\n then \n  u'(c_{it}) = c_{it}^{- \\sigma},\n and thus I get \n  \\frac{c_{jt}(s^t)^{-\\sigma}}{c_{it}(s^t)^{-\\sigma}} = \\frac{\\lambda_i}{\\lambda_j} \\quad \\forall s^t, \\forall t.\n Dropping s^t from the quation for simplicity and taking the logarithms, I obtain \n\\begin{aligned}\n  -\\sigma \\log(c_{jt}) + \\sigma \\log(c_{it}) &= \\log(\\lambda_i) - \\log(\\lambda_j) \\\\\n  \\Leftrightarrow \\log(c_{it}) &= \\frac{1}{\\sigma} \\left( \\log(\\lambda_i) - \\log(\\lambda_j) \\right) + \\log(c_{jt}) \\\\\n  \\Leftrightarrow N \\log(c_{it}) &= \\frac{1}{\\sigma} \\left( N \\log(\\lambda_i) - \\sum_j \\log(\\lambda_j) \\right) + \\sum_j \\log(c_{jt}) \\\\\n  \\Leftrightarrow \\log(c_{it}) &= \\frac{1}{\\sigma} \\left( \\log(\\lambda_i) - \\frac{1}{N} \\sum_j \\log(\\lambda_j) \\right) + \\frac{1}{N} \\sum_j \\log(c_{jt}).\n\\end{aligned}\n By defining the village-average consumption, c_{vt}, as \\log(c_{vt}) = \\frac{1}{N} \\sum_j \\log(c_{jt}), and taking the first difference from the equation above, I get \n  \\log(c_{it}) - \\log(c_{i, t - 1}) = \\log(c_{vt}) - \\log(c_{v, t - 1}).\n\n\n\n2.1.3 Introducing measurement errors\nHere I introduce the measurement error in consumption, which is assumed to be multiplicative and log-normally distributed. In particular, I denote observed consumption, c_{it}^* as c_{it}^* = c_{it} \\exp(\\varepsilon_{it}^c), where \\varepsilon_{it} \\sim N(0, \\gamma_c^2) is the measurement error which is i.i.d. across households and time. With this, the observed village-average consumption, c_{vt}^* is so that \n\\begin{aligned}\n  \\log(c_{vt}^*)\n  &= \\frac{1}{N} \\sum_i \\log(c_{it}^*) \\\\\n  &= \\frac{1}{N} \\sum_i \\log(c_{it}) + \\frac{1}{N} \\sum_i \\varepsilon_{it}^c \\\\\n  &= \\log(c_{vt}) + \\frac{1}{N} \\sum_i \\varepsilon_{it}^c \\\\\n  &= \\log(c_{vt}) + \\varepsilon_{vt}^c,\n\\end{aligned}\n where the village consumption measurement error is defined as \\varepsilon_{vt}^c \\equiv \\frac{1}{N} \\sum_i \\varepsilon_{it}^c. Substituting these into \\log(c_{it}) - \\log(c_{i, t - 1}) = \\log(c_{vt}) - \\log(c_{v, t - 1}), I obtain \n  \\log(c_{it}^*) - \\log(c_{i, t - 1}^*) = \\log(c_{vt}^*) - \\log(c_{v, t - 1}^*) + \\left(\\varepsilon_{it}^c - \\varepsilon_{vt}^c - (\\varepsilon_{i, t - 1}^c - \\varepsilon_{v, t - 1}^c) \\right).\n\n\n\n2.1.4 Likelihood function\nSince \\varepsilon_{it} \\sim N(0, \\gamma_C^2), I get \\varepsilon_{vt} \\sim N(0, \\frac{\\gamma_C^2}{N}), and note that Cov(\\varepsilon_{it}, \\varepsilon_{vt}) = \\frac{\\gamma_C^2}{N}. With the independence across periods and households of measurement errors, I obtain \n  Var(\\varepsilon_{it}^c - \\varepsilon_{vt}^c - (\\varepsilon_{i, t - 1}^c - \\varepsilon_{v, t - 1}^c) )\n  = 2\\left(\\gamma_C^2 + \\frac{\\gamma_C^2}{N} - 2\\frac{\\gamma_C^2}{N} \\right)\n  = 2 \\gamma_C^2 \\left(1 - \\frac{1}{N} \\right).\n\nTherefore, the likelihood function is\n\n  L(\\theta) = \\prod_{i} \\prod_{t = 2}^{T} f \\left( \\log \\left( \\frac{c_{it}^*}{c_{i, t - 1}^*} \\right) - \\log \\left( \\frac{c_{vt}^*}{c_{v, t - 1}^*} \\right), \\theta \\right) ,\n and the log likelihood function is \n  l(\\theta) = \\sum_{i} \\sum_{t = 2}^{T} \\log \\left(f \\left( \\log \\left( \\frac{c_{it}^*}{c_{i, t - 1}^*} \\right) - \\log \\left( \\frac{c_{vt}^*}{c_{v, t - 1}^*} \\right), \\theta\\right) \\right),\n where f(x, \\theta) is the density function of x \\sim N \\left(0, \\sqrt{ 2 \\gamma_C^2 \\left(1 - \\frac{1}{N} \\right)} \\right), and \\theta is a parameter to estimate (here, \\theta = \\gamma_C). In estimation, I find the parameter to maximize l(\\theta).\nWithout assuming that the model is correctly specified but assuming that there is no autocorrelation, the standard error of the parameter is A(\\widehat{\\theta})^{-1} B(\\widehat{\\theta}) A(\\widehat{\\theta})^{-1}, where A(\\widehat{\\theta}) is the Hessian matrix of the log-likelihood function and B(\\widehat{\\theta}) is the outer product of scores (= gradient of log likelihood).\n\n2.1.4.1 Notes\n\nThis model considers that any deviation from the full risk-sharing model is due to measurement errors in consumption.\nAs the paper points out, this model estimates parameters under the assumption that the model follows the full risk-sharing model. This is in contrast to many empirical papers which test a full riks-sharing model (such as Townsend (1994)).\nI derive the unconditional likelihood function, as opposed to the paper which used a conditional likelihood function with simulation. The author clarifies her choice to use the simulation method in a foortnote as follows: “Note that it is not necessary to use simulation to take into account measurement error in consumption at time t in the perfect risk-sharing case. I do it to be consistent with the LC case.”\nI do not rescale the “true” consumption (= consumption without measurement errors) as in the original code, since by the log scale, by construction, the means of true and observed consumption should be identical in expectation.\nInstead of \\log \\left( \\frac{1}{N} \\sum_i c_{it} \\right), I use \\frac{1}{N} \\log \\left( \\sum_i c_{it} \\right) for \\log(c_{vt}) since this is what the model above implies.\nI assume there is no auto correlation when calculating the standard errors since the measurement errors are assumed to be independent across periods."
  },
  {
    "objectID": "laczo2015_full_hom.html#rscript-for-estimation",
    "href": "laczo2015_full_hom.html#rscript-for-estimation",
    "title": "2  Full risk-sharing under homogeneous preferences",
    "section": "3.2 Rscript for estimation",
    "text": "3.2 Rscript for estimation\nThis R script estimates the full risk sharing model.\n\npacman::p_load(\n  tidyverse,\n  kableExtra,\n  numDeriv\n)\n\n\nload('IntermediateData/allData.RData')\n\n\ncalculateLogLikelihoodEachObsFullHom <- function(\n    param, \n    currentCons,\n    previousCons,\n    currentVilLogCons,\n    previousVilLogCons,\n    numHouseholds\n    ) {\n  \n  gammaC2 <- param\n  \n  logLikelihood <- dnorm(\n    (\n      log(currentCons / previousCons)\n      - (currentVilLogCons - previousVilLogCons)\n    ), sd = sqrt(2 * gammaC2 * (1 - 1 / numHouseholds)),\n    log = TRUE\n  )\n  return(logLikelihood)\n}\n\ncalculateScoreEachObsFullHom <- function(\n    param, \n    currentCons,\n    previousCons,\n    currentVilLogCons,\n    previousVilLogCons,\n    numHouseholds\n) {\n  grad(\n    calculateLogLikelihoodEachObsFullHom,\n    param,\n    currentCons = currentCons,\n    previousCons = previousCons,\n    currentVilLogCons = currentVilLogCons,\n    previousVilLogCons = previousVilLogCons,\n    numHouseholds = 34\n  )\n}\n\ncalculateScoreFullHom <- function(\n    param, \n    consdatByVillage,\n    vilMeanLogConsByVillage,\n    numHouseholds,\n    .tnum = tnum\n) {\n  \n  numParameters <- length(param)\n  \n  scoreMatrix <- matrix(0, nrow = numParameters, ncol = numParameters)\n  for (householdIndex in seq(1, numHouseholds)) {\n    for (periodIndex in seq(2, .tnum)) {\n      score <- calculateScoreEachObsFullHom(\n          param,\n          consdatByVillage[householdIndex, periodIndex],\n          consdatByVillage[householdIndex, (periodIndex - 1)],\n          vilMeanLogConsByVillage[periodIndex],\n          vilMeanLogConsByVillage[(periodIndex - 1)],\n          numHouseholds\n        )\n      scoreMatrix <- (\n        scoreMatrix\n        + outer(score, score)\n      )\n    }\n  }\n  return(scoreMatrix)\n}\n\ncalculateLogLikelihoodFullHom <- function(\n    param, \n    consdatByVillage,\n    vilMeanLogConsByVillage,\n    numHouseholds,\n    .tnum = tnum\n    ) {\n  \n  logLikelihood <- 0\n  for (householdIndex in seq(1, numHouseholds)) {\n    for (periodIndex in seq(2, .tnum)) {\n      logLikelihood <- (\n        logLikelihood\n        + calculateLogLikelihoodEachObsFullHom(\n          param,\n          consdatByVillage[householdIndex, periodIndex],\n          consdatByVillage[householdIndex, (periodIndex - 1)],\n          vilMeanLogConsByVillage[periodIndex],\n          vilMeanLogConsByVillage[(periodIndex - 1)],\n          numHouseholds\n        )\n      )\n    }\n  }\n    \n  return(- logLikelihood)\n}\n\ncalculateStandardErrors <- function(\n    estimationResult,\n    consdatByVillage,\n    vilMeanLogConsByVillage,\n    numHouseholds\n) {\n  (\n    solve(estimationResult$hessian) %*% \n    calculateScoreFullHom(\n      estimationResult$par,\n      consdatByVillage, \n      vilMeanLogConsByVillage,\n      numHouseholds\n    ) %*%\n    solve(estimationResult$hessian)\n  ) %>% sqrt %>% diag\n}\n\nestimateMLFullHom <- function(\n    village,\n    consdat,\n    vilMeanLogCons,\n    lowerBound,\n    upperBound,\n    .villageIndicatorMatrix = villageIndicatorMatrix\n) {\n  \n  numHouseholds <- .villageIndicatorMatrix[, village] %>% sum\n  consdatByVillage <- consdat[.villageIndicatorMatrix[, village], ]\n  vilMeanLogConsByVillage <- vilMeanLogCons[village, ]\n  \n  estimationResult <- optim(\n    0.1,\n    calculateLogLikelihoodFullHom,\n    consdatByVillage = consdatByVillage,\n    vilMeanLogConsByVillage = vilMeanLogConsByVillage,\n    numHouseholds = numHouseholds,\n    method = \"L-BFGS-B\",\n    lower = lowerBound,\n    upper = upperBound,\n    control = list(trace = 0, maxit = 200),\n    hessian = TRUE)\n  \n  standardErrors <- calculateStandardErrors(\n    estimationResult,\n    consdatByVillage,\n    vilMeanLogConsByVillage,\n    numHouseholds\n  )\n  \n  return(list(\n    parameter = estimationResult$par,\n    logLikelihood = - estimationResult$value,\n    standardErrors = standardErrors,\n    optimizationResult = estimationResult$message\n  ))\n}\n\nestimationResultFullHom <- map(\n  seq(1, numVillages),\n  ~ estimateMLFullHom(\n      .,\n      consdat,\n      vilMeanLogCons,\n      1e-3,\n      10\n  )\n)"
  },
  {
    "objectID": "laczo2015_dataprep.html#save-data",
    "href": "laczo2015_dataprep.html#save-data",
    "title": "1  Data preparation",
    "section": "1.6 Save data",
    "text": "1.6 Save data\n\nsave.image(\"IntermediateData/allData.RData\")\n\n\n\n\n\nLaczó, Sarolta. 2015. “Risk Sharing with Limited Commitment and Preference Heterogeneity: Structural Estimation and Testing.” Journal of the European Economic Association 13 (2): 265–92."
  },
  {
    "objectID": "laczo2015_full_hom.html#code-for-estimation",
    "href": "laczo2015_full_hom.html#code-for-estimation",
    "title": "2  Full risk-sharing under homogeneous preferences",
    "section": "2.2 Code for estimation",
    "text": "2.2 Code for estimation\nThis R script estimates the full risk sharing model.\n\npacman::p_load(\n  tidyverse,\n  kableExtra,\n  numDeriv\n)\n\n\nload('IntermediateData/allData.RData')\n\n\ncalculateLogLikelihoodEachObsFullHom <- function(\n    param, \n    currentCons,\n    previousCons,\n    currentVilLogCons,\n    previousVilLogCons,\n    numHouseholds\n    ) {\n  \n  gammaC2 <- param\n  \n  logLikelihood <- dnorm(\n    (\n      log(currentCons / previousCons)\n      - (currentVilLogCons - previousVilLogCons)\n    ), sd = sqrt(2 * gammaC2 * (1 - 1 / numHouseholds)),\n    log = TRUE\n  )\n  return(logLikelihood)\n}\n\ncalculateScoreEachObsFullHom <- function(\n    param, \n    currentCons,\n    previousCons,\n    currentVilLogCons,\n    previousVilLogCons,\n    numHouseholds\n) {\n  grad(\n    calculateLogLikelihoodEachObsFullHom,\n    param,\n    currentCons = currentCons,\n    previousCons = previousCons,\n    currentVilLogCons = currentVilLogCons,\n    previousVilLogCons = previousVilLogCons,\n    numHouseholds = numHouseholds\n  )\n}\n\ncalculateScoreFullHom <- function(\n    param, \n    consdatByVillage,\n    vilMeanLogConsByVillage,\n    numHouseholds,\n    .tnum = tnum\n) {\n  \n  numParameters <- length(param)\n  \n  scoreMatrix <- matrix(0, nrow = numParameters, ncol = numParameters)\n  for (householdIndex in seq(1, numHouseholds)) {\n    for (periodIndex in seq(2, .tnum)) {\n      score <- calculateScoreEachObsFullHom(\n          param,\n          consdatByVillage[householdIndex, periodIndex],\n          consdatByVillage[householdIndex, (periodIndex - 1)],\n          vilMeanLogConsByVillage[periodIndex],\n          vilMeanLogConsByVillage[(periodIndex - 1)],\n          numHouseholds\n        )\n      scoreMatrix <- (\n        scoreMatrix\n        + outer(score, score)\n      )\n    }\n  }\n  return(scoreMatrix)\n}\n\ncalculateLogLikelihoodFullHom <- function(\n    param, \n    consdatByVillage,\n    vilMeanLogConsByVillage,\n    numHouseholds,\n    .tnum = tnum\n    ) {\n  \n  logLikelihood <- 0\n  for (householdIndex in seq(1, numHouseholds)) {\n    for (periodIndex in seq(2, .tnum)) {\n      logLikelihood <- (\n        logLikelihood\n        + calculateLogLikelihoodEachObsFullHom(\n          param,\n          consdatByVillage[householdIndex, periodIndex],\n          consdatByVillage[householdIndex, (periodIndex - 1)],\n          vilMeanLogConsByVillage[periodIndex],\n          vilMeanLogConsByVillage[(periodIndex - 1)],\n          numHouseholds\n        )\n      )\n    }\n  }\n    \n  return(- logLikelihood)\n}\n\ncalculateStandardErrors <- function(\n    estimationResult,\n    consdatByVillage,\n    vilMeanLogConsByVillage,\n    numHouseholds\n) {\n  (\n    solve(estimationResult$hessian) %*% \n    calculateScoreFullHom(\n      estimationResult$par,\n      consdatByVillage, \n      vilMeanLogConsByVillage,\n      numHouseholds\n    ) %*%\n    solve(estimationResult$hessian)\n  ) %>% sqrt %>% diag\n}\n\nestimateMLFullHom <- function(\n    village,\n    consdat,\n    vilMeanLogCons,\n    lowerBound,\n    upperBound,\n    villageIndicatorMatrix\n) {\n  \n  numHouseholds <- villageIndicatorMatrix[, village] %>% sum\n  consdatByVillage <- consdat[villageIndicatorMatrix[, village], ]\n  vilMeanLogConsByVillage <- vilMeanLogCons[village, ]\n  \n  estimationResult <- optim(\n    0.1,\n    calculateLogLikelihoodFullHom,\n    consdatByVillage = consdatByVillage,\n    vilMeanLogConsByVillage = vilMeanLogConsByVillage,\n    numHouseholds = numHouseholds,\n    method = \"L-BFGS-B\",\n    lower = lowerBound,\n    upper = upperBound,\n    control = list(trace = 0, maxit = 200),\n    hessian = TRUE)\n  \n  standardErrors <- calculateStandardErrors(\n    estimationResult,\n    consdatByVillage,\n    vilMeanLogConsByVillage,\n    numHouseholds\n  )\n  \n  return(list(\n    parameter = estimationResult$par,\n    logLikelihood = - estimationResult$value,\n    standardErrors = standardErrors,\n    optimizationResult = estimationResult$message\n  ))\n}\n\nestimationResultFullHom <- map(\n  seq(1, numVillages),\n  ~ estimateMLFullHom(\n      .,\n      consdat,\n      vilMeanLogCons,\n      1e-3,\n      10,\n      villageIndicatorMatrix\n  )\n)"
  },
  {
    "objectID": "laczo2015_lc_hom_model.html#model",
    "href": "laczo2015_lc_hom_model.html#model",
    "title": "3  Risk sharing with limited commitoment: model",
    "section": "3.1 Model",
    "text": "3.1 Model\nI consider constrained-efficient consumption allocations. The social planner solves the following problem:\n\\begin{align*}\n  &\\max_{\\{c_{it}(s^t)\\}} \\sum_i \\lambda_i \\sum_{t = 1}^{\\infty} \\sum_{s^t} \\delta^t \\pi(s^t) u(c_{it}(s^t)) \\\\\n  \\text{subject to}\n  &\\sum_i c_{it} (s^t) \\le \\sum_i y_{it}(s_t) \\quad \\forall s^t, \\forall t \\\\\n  &\\sum_{r = t}^{\\infty} \\sum_{s^r} \\delta^{r - t} \\pi(s^r | s^t) u(c_{ir}(s^r)) \\ge U_{i}^{aut}(s_t) \\quad \\forall s^t, \\forall t, \\forall i.\n\\end{align*}\nHere, the income follows a Markov process and is independent across households. Notice the difference between the history of states up to period t (s^t) and the state at period t (s_t). The variable \\lambda_i is the Pareto weight of a household i. The last equation is the participation constraints (PCs), whose RHS is the value of autarky and the solution of the following Bellman equation:\n\n  U_i^{aut}(s_t) = u((1 - \\phi) y_{it}(s_t)) + \\delta \\sum_{s^{t + 1}} \\pi(s_{t + 1} | s_t) U_{i}^{aut}(s_{t + 1}),\n where \\phi is the punishment of renege, which is a fraction of consumption each period. It is assumed that savings are absent.\nLetting the multiplier on the PC of i be \\delta^t \\pi(s^t) \\mu_i(s^t) and the multiplier on the aggregate resource constraint be \\delta^t \\pi(s^t) \\rho(s^t), the Lagrangian is\n\n  \\sum_{t = 1}^{\\infty} \\sum_{s^t} \\delta^t \\pi(s^t) \\left\\{ \\sum_i \\left[ \\lambda_i u(c_{it}(s^t)) + \\mu_i(s^t) \\left( \\sum_{r = t}^{\\infty} \\sum_{s^r} \\delta^{r - t} \\pi(s^r | s^t) u (c_{ir} (s^r)) - U_i^{aut}(s_t) \\right) \\right] + \\rho(s^t) \\left( \\sum_i \\left(y_{it} (s_t) - c_{it} (s^t) \\right) \\right) \\right\\}\n With the recursive method in Marcet and Marimon (2019), this Lagrangian can be written as\n\n  \\sum_{t = 1}^{\\infty} \\sum_{s^t} \\delta^t \\pi(s^t) \\left\\{ \\sum_i \\left[ M_i (s^{t - 1}) u (c_{it} (s^t)) + \\mu_i (s^t) (u (c_{it} (s^t)) - U_i^{aut} (s_t)) \\right] + \\rho(s^t) \\left( \\sum_i \\left( y_{it}(s_t) - c_{it} (s^t) \\right) \\right) \\right\\},\n\nwhere M_i(s^t) = M_i(s^{t - 1}) + \\mu_i(s^t) and M_i(s^0) = \\lambda. The variable M_i(s^t) is the current Pareto weight of household i and is equal to its initial Pareto weight plus the sum of the Lagrange mulipliers on its PCs along the history s^t.\nFrom the Lagrangian, the optimality condition is \n  u'(c_{it}(s^t)) M_i(s^t) = \\frac{\\rho(s^t)}{\\delta^t \\pi(s^t)},\n and thus, for two households i and j (i \\ne j), \n  u'(c_{it}(s^t)) M_i(s^t) = u'(c_{jt}(s^t)) M_j(s^t).\n Taking logarithms and summing over households j \\ne i, I get \n\\begin{aligned}\n  \\log \\left(u'(c_{it}(s^t)) \\right) + \\log \\left(M_i(s^t) \\right) = \\frac{1}{N - 1} \\sum_{j \\ne i} \\log \\left(u'(c_{jt}(s^t)) \\right) + \\frac{1}{N - 1} \\sum_{j \\ne i} \\log \\left(M_j(s^t) \\right).\n\\end{aligned}\n\nThe pareto weights of N-households are intractable and the dimension of the value function becomes too large. Hence, I take the “one-versus-the-rest” approach, as in Laczó (2015) and Ligon, Thomas, and Worrall (2002). In particular, assume that “the rest of the village” has the same consumption (c_{vt}(s^t)) and Pareto weights (M_v(s^t)). Then the equation above becomes\n\n\\begin{aligned}\n  \\log \\left(u'('c_{it}(s^t)) \\right) + \\log \\left(M_i(s^t) \\right) &= \\log \\left(u'(c_{vt}(s^t)) \\right) + \\log \\left( M_v(s^t) \\right) \\\\\n  \\Leftrightarrow \\frac{u'(c_{vt}(s^t))}{u'(c_{it}(s^t))} = \\frac{M_i(s^t)}{M_v(s^t)}\n\\end{aligned}\n Note that this is equivalent to the optimality condition that the ratio of marginal utilities between two “households”, i and v, equals the ratio of their Pareto weights.\nLet x_i(s^t) = \\frac{M_i(s^t)}{M_v(s^t)}, the relative Pareto weight of household i under the history s^t. Then, the vector of relative weights x(s^t) plays as a role as a co-state variable, and the solution consists of policy functions x_{it}(s_t, x_{t - 1}) and c_{it}(s_t, x_{t - 1}). That is, x_{t - 1} is a sufficient statistic for the history up to t - 1. The optimality condition is\n\n  \\frac{u'(c_{vt}(s_t, x_{t - 1}))}{u'(c_{it}(s_t, x_{t - 1}))} = x_{it}(s_t, x_{t - 1}) \\quad \\forall i.\n\nUsing the policy functions, the individual value functioon can be written recursively as\n\n  V_i(s_t, x_{t - 1}) = u (c_{it} (s_t, x_{t - 1})) + \\delta \\sum_{s_{t + 1}} \\pi(s_{t + 1} | s_t) V_i (s_{t + 1}, x_t(s_t, x_{t - 1})).\n\nThe evolution of relative Pareto weights is fully characterized by state-dependent intervals, which give the weights in the case where PCs are binding (Ligon, Thomas, and Worrall (2002))."
  },
  {
    "objectID": "laczo2015_lc_hom_model.html#code",
    "href": "laczo2015_lc_hom_model.html#code",
    "title": "3  Risk sharing with limited commitoment: model",
    "section": "3.2 Code",
    "text": "3.2 Code\n\npacman::p_load(\n  tidyverse,\n  kableExtra,\n  latex2exp\n)\n\n\n3.2.1 Utility functions\n\ncalculateUtility <- function(cons, sigma) {\n  if (sigma != 1) {\n    utility = (cons^(1 - sigma) - 1) / (1 - sigma)\n  } else if (sigma == 1) {\n    utility = log(cons)\n  }\n  return(utility)\n}\ncalculateMarginalUtility <- function(cons, sigma) cons^(- sigma)\n\n\n\n3.2.2 Value under autarky\n\ncalculateAutarkyValue <- function(\n    incomeGridPoints, \n    sigma,\n    delta,\n    punishment,\n    incomeTransitionMatrix\n) {\n  \n  autarkyValue <- numeric(length = length(incomeGridPoints))\n  i <- 1\n  diff <- 1\n  while (diff > 1e-12) {\n    autarkyValueNew <- (\n      calculateUtility(incomeGridPoints * (1 - punishment), sigma) \n      + delta * incomeTransitionMatrix %*% autarkyValue\n    )\n    diff <- max(abs(autarkyValueNew - autarkyValue))\n    autarkyValue <- autarkyValueNew\n    i <- i + 1\n  }\n  return(autarkyValue)\n}\n\n\n\n3.2.3 Create grid of relative Pareto weights\nHere, I make the grid of relative Pareto weight of the household, x(s^t), on which I compute the values (I use the notation x(s^t) rather than x_i(s^t) since there are only two households).\n\ngetRelativeParetoWeightsGridPoints <- function(\n    sigma,\n    punishment,\n    householdIncomeGridPoints,\n    villageIncomeGridPoints,\n    numRelativeParetoWeights\n    ) {\n  \n  minRelativeParetoWeights <- (\n    calculateMarginalUtility(max(villageIncomeGridPoints), sigma) \n    / calculateMarginalUtility(min(householdIncomeGridPoints * (1 - punishment)), sigma)\n  )\n  maxRelativeParetoWeights <- (\n    calculateMarginalUtility(min(villageIncomeGridPoints * (1 - punishment)), sigma) \n    / calculateMarginalUtility(max(householdIncomeGridPoints), sigma)\n  )\n  relativeParetoWeightsGridPoints <- exp(\n    seq(\n      log(minRelativeParetoWeights), \n      log(maxRelativeParetoWeights), \n      length.out = numRelativeParetoWeights)\n    )\n  return(relativeParetoWeightsGridPoints)\n}\n\n\n\n3.2.4 Calculate consumption on the grid points\nThen, I compute consumptions of the household on these grid points. From the optimality condition and the CRRA utility functions, we obtain\n\n\\begin{aligned}\n  \\frac{c_{vt}^{-\\sigma}}{c_{it}^{-\\sigma}} &= x_{t} \\\\\n  \\Leftrightarrow c_{vt} &= c_{it} x_t^{-1/\\sigma} \\\\\n  \\Leftrightarrow (N - 1) c_{vt} &= (N - 1) c_{it} x_t^{-1/\\sigma} \\\\\n  \\Leftrightarrow c_{it} + (N - 1) c_{vt} &= c_{it} \\left( 1 + (N - 1) x_t^{-1/\\sigma} \\right) \\\\\n  \\Leftrightarrow c_{it} &= \\frac{y_t}{ \\left( 1 + (N - 1) x_t^{-1/\\sigma} \\right)},\n\\end{aligned}\n where y_t is the village aggregate income and the last transformation used the village wide equals the aggregate income due to the no saving assumption.\n\ncalculateHouseholdConsumption <- function(\n  aggregateIncome,\n  relativeParetoWeight,\n  numHouseholds,\n  sigma\n) {\n    aggregateIncome / (1 + (numHouseholds - 1) * (relativeParetoWeight^(- 1 / sigma)))\n}\n\n\n\n3.2.5 Caclulate values under full risk-sharing\nNow, I compute the values under full risk-sharing, which will be used as the initial values in value function iterations under the limited commitment model. Note that, under full risk sharing, the consumption only depends on the aggregate resources and time-invariate relative Pareto weights. Hence, I numerically solve the following Bellman equation:\n\n  V_i^{full}(s_t, x) = u(c_{it}(s_t, x)) + \\delta \\sum_{s^{t + 1}} \\pi(s_{t + 1} | s_t) V_{i}^{full}(s_{t + 1}, x).\n\n\ncalculateValueFullRiskSharing <- function(\n  incomeTransitionMatrix, \n  aggregateIncomeGridPoints, \n  delta, \n  sigma, \n  autarkyValueMatrix, \n  consumptionOnRelativeParetoWeightGrid,\n  numRelativeParetoWeights,\n  numHouseholds\n  ) {\n\n  # Initial guess is expected utilities under autarky\n  householdValueFullRiskSharing <- outer(\n    autarkyValueMatrix[, 1], rep(1, numRelativeParetoWeights)\n    )\n  villageValueFullRiskSharing <- outer(\n    autarkyValueMatrix[, 2], rep(1, numRelativeParetoWeights)\n    )\n\n  iteration <- 1\n  diff <- 1\n  while (diff > 1e-10 & iteration < 500) {\n    householdValueFullRiskSharingNew <- (\n      calculateUtility(consumptionOnRelativeParetoWeightGrid, sigma) \n      + delta * incomeTransitionMatrix %*% householdValueFullRiskSharing\n    )\n    villageValueFullRiskSharingNew <- (\n      calculateUtility(\n        (aggregateIncomeGridPoints - consumptionOnRelativeParetoWeightGrid) / (numHouseholds - 1), \n        sigma\n        ) \n      + delta * incomeTransitionMatrix %*% villageValueFullRiskSharing\n    )\n    \n    diff <- max(\n      max(abs(householdValueFullRiskSharing - householdValueFullRiskSharingNew)), \n      max(abs(villageValueFullRiskSharing - villageValueFullRiskSharingNew))\n      )\n    householdValueFullRiskSharing <- householdValueFullRiskSharingNew\n    villageValueFullRiskSharing <- villageValueFullRiskSharingNew\n    iteration <- iteration + 1\n    \n  }\n\n  return(list(\n    householdValueFullRiskSharing = householdValueFullRiskSharing, \n    villageValueFullRiskSharing = villageValueFullRiskSharing\n    ))\n}\n\n\n\n3.2.6 Calculate values under risk-sharing with limited commitment\nNext, I derive the state-dependent intervals of relative Pareto weights and calculate values under the model of limited commitment. To derive the intervals, I use the fact that at the limits of the intervals, the PCs are binding. For instance, to compute the lower limit \\underline{x}^h(s), where h indicates h’th iteration, the PC of the household is binding:\n\n  u(c_i^h(s)) + \\delta \\sum_{s'} \\pi(s' | s) V_i^{h - i} (s', \\underline{x}^h(s)) = U_i^{aut}(s),\n where the optimality condition is\n\n  \\frac{u'(c_{2}^h(s))}{u'(c_{1}^h(s))} = \\underline{x}^h(s).\n\nNotice that, once the PC binds, the past history, which is summarized by x_{t - 1}, does not matter. This property is called “amnesia” (Kocherlakota (1996)) or “forgiveness” (Ligon, Thomas, and Worrall (2002)). Since \\underline{x}^h(s) may not be on the grid q, linear interpolation is used to compute V_i^{h - 1}(s', \\underline{x}^h(s)).\nSimilarly, \\overline{x}^h(s) is computed using the binding PC of the village:\n\n  u(c_v^h(s)) + \\delta \\sum_{s'} \\pi(s' | s) V_v^{h - i} (s', \\overline{x}^h(s)) = U_v^{aut}(s),\n where the optimality condition is\n\n  \\frac{u'(c_{2}^h(s))}{u'(c_{1}^h(s))} = \\overline{x}^h(s).\n\nAfter deriving these limits of intervals,\n\nfor relative Pareto weights below \\underline{x}^h(s), compute consumption of the household based on \\underline{x}^h(s) and let its value be U_i^{aut};\nfor relative Pareto weights above \\overline{x}^h(s), compute consumption of the household based on \\overline{x}^h(s) and let the value of village be U_v^{aut};\nfor other relative Pareto weights, use them to compute consumption of the household and the values of the household and village.\n\nBy iterating these steps, we can calculate the value functions of households and limits of intervals.\n\ninterpolateValueFunction <- function(\n    relativeParetoWeight,\n    relativeParetoWeightsGridPoints,\n    valueFunctionMatrix\n    ) {\n  apply(\n    valueFunctionMatrix,\n    1,\n    function(x) {\n      approx(\n        relativeParetoWeightsGridPoints, \n        x, \n        relativeParetoWeight,\n        rule = 2\n        )$y\n    }\n    )\n}\n\ncalculateDiffLCRiskSharingAndAutarky <- function(\n    relativeParetoWeight,\n    relativeParetoWeightsGridPoints,\n    delta,\n    sigma,\n    aggregateIncome,\n    householdValueLCRiskSharing,\n    villageValueLCRiskSharing,\n    incomeTransitionProbVec,\n    householdAutarkyValue,\n    villageAutarkyValue,\n    numHouseholds\n    ) {\n  \n  householdConsumption <- calculateHouseholdConsumption(\n    aggregateIncome,\n    relativeParetoWeight,\n    numHouseholds,\n    sigma\n  )\n  \n  householdValueLCRiskSharingAtRelativeParetoWeight <- interpolateValueFunction(\n    relativeParetoWeight,\n    relativeParetoWeightsGridPoints,\n    householdValueLCRiskSharing\n    )\n  villageValueLCRiskSharingAtRelativeParetoWeight <- interpolateValueFunction(\n    relativeParetoWeight,\n    relativeParetoWeightsGridPoints,\n    villageValueLCRiskSharing\n    )\n  \n  householdDiffLCRiskSharingAndAutarky <- (\n    calculateUtility(householdConsumption, sigma) \n    + delta * incomeTransitionProbVec %*% householdValueLCRiskSharingAtRelativeParetoWeight \n    - householdAutarkyValue\n  ) %>% as.numeric\n  villageDiffLCRiskSharingAndAutarky <- (\n    calculateUtility((aggregateIncome - householdConsumption) / (numHouseholds - 1), sigma) \n    + delta * incomeTransitionProbVec %*% villageValueLCRiskSharingAtRelativeParetoWeight \n    - villageAutarkyValue\n  ) %>% as.numeric\n\n  return(list(\n    householdDiffLCRiskSharingAndAutarky = householdDiffLCRiskSharingAndAutarky,\n    villageDiffLCRiskSharingAndAutarky = villageDiffLCRiskSharingAndAutarky\n  ))\n}\n\ngetClosestGridIndex <- function(\n  point,\n  gridPoints\n) {\n  closestGridIndex <- which.min(\n    abs(point - gridPoints) \n  )\n  if (\n    point \n    > gridPoints[closestGridIndex]\n    ) {\n    closestGridIndex <- closestGridIndex + 1\n  }\n  return(closestGridIndex)\n}\n\ncalculateValueLCRiskSharing <- function(\n  valueFullRiskSharing,\n  consumptionOnRelativeParetoWeightGrid,\n  aggregateIncomeGridPoints,\n  incomeTransitionMatrix,\n  autarkyValueMatrix,\n  relativeParetoWeightsGridPoints,\n  numRelativeParetoWeights,\n  delta,\n  sigma,\n  numIncomeStates,\n  numHouseholds,\n  iterationLimit,\n  diffLimit\n) {\n  \n  # Initial guess is expected utilities under full risk sharing\n  householdValueLCRiskSharing <- valueFullRiskSharing$householdValueFullRiskSharing\n  villageValueLCRiskSharing <- valueFullRiskSharing$villageValueFullRiskSharing\n  \n  diff <- 1\n  iteration <- 1\n  while ((diff > diffLimit) && (iteration <= iterationLimit)) {\n    \n    # First, ignore enforceability and just update the value functions\n    # using the values at the previous iteration\n    householdValueLCRiskSharingNew <- (\n      calculateUtility(consumptionOnRelativeParetoWeightGrid, sigma) \n      + delta * incomeTransitionMatrix %*% householdValueLCRiskSharing\n    )\n    villageValueLCRiskSharingNew <- (\n      calculateUtility(\n        (aggregateIncomeGridPoints - consumptionOnRelativeParetoWeightGrid) / (numHouseholds - 1), \n        sigma\n        )\n      + delta * incomeTransitionMatrix %*% villageValueLCRiskSharing\n    )\n    \n    # Now check enforceability at each state\n    for (incomeStateIndex in seq(1, numIncomeStates)) {\n      householdAutarkyValue <- autarkyValueMatrix[incomeStateIndex, 1]\n      villageAutarkyValue <- autarkyValueMatrix[incomeStateIndex, 2]\n      \n      if (any(householdValueLCRiskSharingNew[incomeStateIndex, ] <= householdAutarkyValue)) {\n        villageValueLCRiskSharingNew[\n          incomeStateIndex,\n          householdValueLCRiskSharingNew[incomeStateIndex, ] <= householdAutarkyValue\n        ] <- villageValueLCRiskSharingNew[\n          incomeStateIndex,\n          householdValueLCRiskSharingNew[incomeStateIndex, ] <= householdAutarkyValue\n        ] %>% min\n        householdValueLCRiskSharingNew[\n          incomeStateIndex,\n          householdValueLCRiskSharingNew[incomeStateIndex, ] <= householdAutarkyValue\n        ] <- householdAutarkyValue\n      }\n      \n      if (any(villageValueLCRiskSharingNew[incomeStateIndex, ] <= villageAutarkyValue)) {\n        householdValueLCRiskSharingNew[\n          incomeStateIndex,\n          villageValueLCRiskSharingNew[incomeStateIndex, ] <= villageAutarkyValue\n        ] <- householdValueLCRiskSharingNew[\n          incomeStateIndex,\n          villageValueLCRiskSharingNew[incomeStateIndex, ] <= villageAutarkyValue\n        ] %>% min\n        villageValueLCRiskSharingNew[\n          incomeStateIndex,\n          villageValueLCRiskSharingNew[incomeStateIndex, ] <= villageAutarkyValue\n        ] <- villageAutarkyValue\n      }\n    }\n      \n    diff <- max(\n      max(abs(householdValueLCRiskSharingNew - householdValueLCRiskSharing)),\n      max(abs(villageValueLCRiskSharingNew - villageValueLCRiskSharing))\n    )\n    householdValueLCRiskSharing <- householdValueLCRiskSharingNew\n    villageValueLCRiskSharing <- villageValueLCRiskSharingNew\n    iteration <- iteration + 1\n  }\n  \n  relativeParetoWeightBounds <- matrix(NA, nrow = numIncomeStates, ncol = 2)\n  \n  for (incomeStateIndex in seq(1, numIncomeStates)) {\n    aggregateIncome <- aggregateIncomeGridPoints[incomeStateIndex]\n    incomeTransitionProbVec <- incomeTransitionMatrix[incomeStateIndex,]\n    householdAutarkyValue <- autarkyValueMatrix[incomeStateIndex, 1]\n    villageAutarkyValue <- autarkyValueMatrix[incomeStateIndex, 2]\n\n    if (\n      calculateDiffLCRiskSharingAndAutarky(\n        min(relativeParetoWeightsGridPoints),\n        relativeParetoWeightsGridPoints,\n        delta,\n        sigma,\n        aggregateIncome,\n        householdValueLCRiskSharing,\n        villageValueLCRiskSharing,\n        incomeTransitionProbVec,\n        householdAutarkyValue,\n        villageAutarkyValue,\n        numHouseholds\n        )$householdDiffLCRiskSharingAndAutarky < 0) {\n        relativeParetoWeightLowerBound <- uniroot(\n          function(x) {calculateDiffLCRiskSharingAndAutarky(\n          x,\n          relativeParetoWeightsGridPoints,\n          delta,\n          sigma,\n          aggregateIncome,\n          householdValueLCRiskSharing,\n          villageValueLCRiskSharing,\n          incomeTransitionProbVec,\n          householdAutarkyValue,\n          villageAutarkyValue,\n          numHouseholds\n          )$householdDiffLCRiskSharingAndAutarky}, \n        c(min(relativeParetoWeightsGridPoints), max(relativeParetoWeightsGridPoints)), \n        tol = 1e-10, \n        maxiter = 300\n        )$root\n        } else {\n          relativeParetoWeightLowerBound <- min(relativeParetoWeightsGridPoints)\n        }\n    \n    if (\n      calculateDiffLCRiskSharingAndAutarky(\n        max(relativeParetoWeightsGridPoints),\n        relativeParetoWeightsGridPoints,\n        delta,\n        sigma,\n        aggregateIncome,\n        householdValueLCRiskSharing,\n        villageValueLCRiskSharing,\n        incomeTransitionProbVec,\n        householdAutarkyValue,\n        villageAutarkyValue,\n        numHouseholds\n        )$villageDiffLCRiskSharingAndAutarky < 0) {\n        relativeParetoWeightUpperBound <- uniroot(\n          function(x) {calculateDiffLCRiskSharingAndAutarky(\n          x,\n          relativeParetoWeightsGridPoints,\n          delta,\n          sigma,\n          aggregateIncome,\n          householdValueLCRiskSharing,\n          villageValueLCRiskSharing,\n          incomeTransitionProbVec,\n          householdAutarkyValue,\n          villageAutarkyValue,\n          numHouseholds\n          )$villageDiffLCRiskSharingAndAutarky}, \n        c(min(relativeParetoWeightsGridPoints), max(relativeParetoWeightsGridPoints)), \n        tol = 1e-10, \n        maxiter = 300\n        )$root\n        } else {\n          relativeParetoWeightUpperBound <- max(relativeParetoWeightsGridPoints)\n        }\n        relativeParetoWeightBounds[incomeStateIndex, 1] <- relativeParetoWeightLowerBound\n        relativeParetoWeightBounds[incomeStateIndex, 2] <- relativeParetoWeightUpperBound\n  }\n  \n  if (iteration == iterationLimit) {\n    print(\"Reached the maximum limit of iterations!\")\n  }\n  \n  return(list(\n    householdValueLCRiskSharing = householdValueLCRiskSharing,\n    villageValueLCRiskSharing = villageValueLCRiskSharing,\n    relativeParetoWeightBounds = relativeParetoWeightBounds))\n}\n\n\nsolveLCRiskSharing <- function(\n    delta,\n    sigma,\n    punishment,\n    householdIncomeTransitionMatrix,\n    householdIncomeGridPoints,\n    villageIncomeTransitionMatrix,\n    villageIncomeGridPoints,\n    numIncomeStates,\n    numHouseholds,\n    numRelativeParetoWeights = 2000,\n    iterationLimit = 100,\n    diffLimit = 1e-8\n){\n  \n  incomeTransitionMatrix <- kronecker(\n    villageIncomeTransitionMatrix,\n    householdIncomeTransitionMatrix\n    )\n  \n  incomeGridPointsMatrix <- as.matrix(expand.grid(\n    householdIncomeGridPoints, villageIncomeGridPoints\n    ))\n  \n  aggregateIncomeGridPoints <- (\n    incomeGridPointsMatrix[, 1] + incomeGridPointsMatrix[, 2] * (numHouseholds - 1)\n  )\n  \n  autarkyValueMatrix <- expand.grid(\n    calculateAutarkyValue(\n      householdIncomeGridPoints,\n      sigma,\n      delta,\n      punishment,\n      householdIncomeTransitionMatrix\n    ),\n    calculateAutarkyValue(\n      villageIncomeGridPoints,\n      sigma,\n      delta,\n      punishment,\n      villageIncomeTransitionMatrix\n    )\n  )\n  \n  relativeParetoWeightsGridPoints <- getRelativeParetoWeightsGridPoints(\n      sigma,\n      punishment,\n      householdIncomeGridPoints,\n      villageIncomeGridPoints,\n      numRelativeParetoWeights\n      )\n  \n  consumptionOnRelativeParetoWeightGrid <- matrix(\n    NA, nrow = numIncomeStates, ncol = numRelativeParetoWeights\n    )\n  for (incomeStateIndex in seq_along(aggregateIncomeGridPoints)) {\n    for (relativeParetoWeightIndex in seq_along(relativeParetoWeightsGridPoints)) {\n      consumptionOnRelativeParetoWeightGrid[\n        incomeStateIndex, \n        relativeParetoWeightIndex\n        ] <- calculateHouseholdConsumption(\n          aggregateIncomeGridPoints[incomeStateIndex],\n          relativeParetoWeightsGridPoints[relativeParetoWeightIndex],\n          numHouseholds,\n          sigma\n        )\n      }\n    }\n\n  valueFullRiskSharing <- calculateValueFullRiskSharing(\n    incomeTransitionMatrix, \n    aggregateIncomeGridPoints, \n    delta, \n    sigma, \n    autarkyValueMatrix, \n    consumptionOnRelativeParetoWeightGrid,\n    numRelativeParetoWeights,\n    numHouseholds\n    )\n\n  valueLCRiskSharing <- calculateValueLCRiskSharing(\n    valueFullRiskSharing,\n    consumptionOnRelativeParetoWeightGrid,\n    aggregateIncomeGridPoints,\n    incomeTransitionMatrix,\n    autarkyValueMatrix,\n    relativeParetoWeightsGridPoints,\n    numRelativeParetoWeights,\n    delta,\n    sigma,\n    numIncomeStates,\n    numHouseholds,\n    iterationLimit,\n    diffLimit\n  )\n\n  return(valueLCRiskSharing)\n}\n\n\nsolveLCRiskSharingByVillage <- function(\n    village,\n    delta,\n    sigma,\n    punishment,\n    householdAR1EstimationResult,\n    villageAR1EstimationResult,\n    numIncomeStates,\n    numRelativeParetoWeights = 2000,\n    iterationLimit = 100,\n    diffLimit = 1e-8\n) {\n  \n  numHouseholds <- householdAR1EstimationResult[[village]]$numHouseholds\n  \n  relativeParetoWeightBoundsArray <- array(\n    NA,\n    c(2, 2, numIncomeStates, 2)\n  )\n  \n  for (meanClass in seq(1, 2)) {\n    for (CVClass in seq(1, 2)) {\n      householdIncomeTransitionMatrix <- (\n        householdAR1EstimationResult[[village]]$transitionMatrixArray[meanClass, CVClass, ,]\n      )\n      householdIncomeGridPoints <- (\n        householdAR1EstimationResult[[village]]$gridPointsArray[meanClass, CVClass,]\n      )\n      villageIncomeTransitionMatrix <- (\n        villageAR1EstimationResult[[village]]$transitionMatrix\n      )\n      villageIncomeGridPoints <- (\n        villageAR1EstimationResult[[village]]$gridPoints\n      )\n  \n      relativeParetoWeightBoundsArray[meanClass, CVClass, ,] <- solveLCRiskSharing(\n        delta,\n        sigma,\n        punishment,\n        householdIncomeTransitionMatrix,\n        householdIncomeGridPoints,\n        villageIncomeTransitionMatrix,\n        villageIncomeGridPoints,\n        numIncomeStates,\n        numHouseholds,\n        numRelativeParetoWeights,\n        iterationLimit,\n        diffLimit\n      )$relativeParetoWeightBounds\n    }\n  }\n  return(relativeParetoWeightBoundsArray)\n}\n\n\n\n\n\nsave.image(\"IntermediateData/lc_hom_model_functions.RData\")\n\n\n\n3.2.7 Sanity test: replication of Figure 1 in Ligon, Thomas, and Worrall (2002)\nFor the sanity test of this function, I use it to replicate the Figure 1 in Ligon, Thomas, and Worrall (2002). Here I use the parameter values in the original paper. I choose the income process (y_l, y_h) = (2/3, 4/3) and (p_l, p_h) = (0.1, 0.9) for both households so that the mean is 1 and the ratio y_l / y_h is 1/2 as in the paper. Also, the penalty under autarky is absent as in the original numerical exercise. Finally, I assume the CRRA utility functions:\n\n  u(c_{it}) = \\frac{c_{it}^{1 - \\sigma} - 1}{1 - \\sigma}.\n\n\nsigmaLTW <- 1.0\npunishmentLTW <- 0.0\n\nincomeTransitionMatrixLTW <- matrix(rep(c(0.1, 0.9), 2), nrow = 2, byrow = TRUE)\nincomeGridPointsLTW <- c(2/3, 4/3)\nnumIncomeStatesLTW <- length(incomeGridPointsLTW) *  length(incomeGridPointsLTW)\nnumHouseholdsLTW <- 2\n\ndeltaVec <- seq(0.8, 0.999, by = 0.002)\n\n\nLCRiskSharingResultLTW <- map(\n  deltaVec,\n  ~ solveLCRiskSharing(\n    .,\n    sigmaLTW,\n    punishmentLTW,\n    incomeTransitionMatrixLTW,\n    incomeGridPointsLTW,\n    incomeTransitionMatrixLTW,\n    incomeGridPointsLTW,\n    numIncomeStatesLTW,\n    numHouseholdsLTW,\n    numRelativeParetoWeights = 10000,\n    iterationLimit = 1000,\n    diffLimit = 1e-8\n    )\n)\n\n\n\n\n\n\n\n\nrelativeParetoWeightBoundsArrayLTW = array(\n  NA, \n  dim = c(numIncomeStatesLTW, 2, length(deltaVec))\n  )\n\nfor (deltaIndex in seq_along(deltaVec)) {\n  relativeParetoWeightBoundsArrayLTW[,,deltaIndex] <- (\n    LCRiskSharingResultLTW[[deltaIndex]]$relativeParetoWeightBounds\n  )\n}\n\nggplot() +\n  geom_line(aes(deltaVec, log(relativeParetoWeightBoundsArrayLTW[1,1,]), color = \"a\")) +\n  geom_line(aes(deltaVec, log(relativeParetoWeightBoundsArrayLTW[1,2,]), color = \"b\")) +\n  geom_line(aes(deltaVec, log(relativeParetoWeightBoundsArrayLTW[2,1,]), color = \"c\")) +\n  geom_line(aes(deltaVec, log(relativeParetoWeightBoundsArrayLTW[2,2,]), color = \"d\")) +\n  geom_line(aes(deltaVec, log(relativeParetoWeightBoundsArrayLTW[3,1,]), color = \"e\")) +\n  geom_line(aes(deltaVec, log(relativeParetoWeightBoundsArrayLTW[3,2,]), color = \"f\")) +\n  geom_line(aes(deltaVec, log(relativeParetoWeightBoundsArrayLTW[4,1,]), color = \"g\")) +\n  geom_line(aes(deltaVec, log(relativeParetoWeightBoundsArrayLTW[4,2,]), color = \"h\")) +\n  coord_cartesian(\n    xlim = c(0.8, 1.0), \n    ylim = c(\n      log(incomeGridPointsLTW[1] / incomeGridPointsLTW[2]),\n      log(incomeGridPointsLTW[2] / incomeGridPointsLTW[1])\n      )\n    ) +\n  geom_ribbon(aes(x = deltaVec,\n                  ymin = log(relativeParetoWeightBoundsArrayLTW[1,1,]),\n                  ymax = log(relativeParetoWeightBoundsArrayLTW[1,2,])),\n                  fill = \"blue\", alpha = 0.2) +\n  geom_ribbon(aes(x = deltaVec,\n                  ymin = log(relativeParetoWeightBoundsArrayLTW[2,1,]),\n                  ymax = log(relativeParetoWeightBoundsArrayLTW[2,2,])),\n                  fill = \"red\", alpha = 0.2) +\n  geom_ribbon(aes(x = deltaVec,\n                  ymin = log(relativeParetoWeightBoundsArrayLTW[3,1,]),\n                  ymax = log(relativeParetoWeightBoundsArrayLTW[3,2,])),\n                  fill = \"green\", alpha = 0.2) +\n  geom_ribbon(aes(x = deltaVec,\n                  ymin = log(relativeParetoWeightBoundsArrayLTW[4,1,]),\n                  ymax = log(relativeParetoWeightBoundsArrayLTW[4,2,])),\n                  fill = \"yellow\", alpha = 0.2) +\n  scale_color_manual(\n    name = \"End-points\",\n    values = c(\n      \"blue\",\n      \"purple\",\n      \"brown\",\n      \"red\",\n      \"yellow\",\n      \"green\",\n      \"orange\",\n      \"gray\"\n      ),\n    labels = unname(TeX(c(\n      \"$\\\\underline{x}_{ll}$\",\n      \"$\\\\bar{x}_{ll}$\",\n      \"$\\\\underline{x}_{hl}$\",\n      \"$\\\\bar{x}_{hl}$\",\n      \"$\\\\underline{x}_{lh}$\",\n      \"$\\\\bar{x}_{lh}$\",\n      \"$\\\\underline{x}_{hh}$\",\n      \"$\\\\bar{x}_{hh}$\"\n      )))\n    ) +\n  xlab(\"Discount factor (delta)\") +\n  ylab(\"log of the relative Pareto weights (x)\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nKocherlakota, N. R. 1996. “Implications of Efficient Risk Sharing without Commitment.” The Review of Economic Studies 63 (4): 595–609. https://doi.org/10.2307/2297795.\n\n\nLaczó, Sarolta. 2015. “Risk Sharing with Limited Commitment and Preference Heterogeneity: Structural Estimation and Testing.” Journal of the European Economic Association 13 (2): 265–92.\n\n\nLigon, Ethan, Jonathan P. Thomas, and Tim Worrall. 2002. “Informal Insurance Arrangements with Limited Commitment: Theory and Evidence from Village Economies.” Review of Economic Studies 69 (1): 209–44. https://doi.org/10.1111/1467-937X.00204.\n\n\nMarcet, Albert, and Ramon Marimon. 2019. “Recursive Contracts.” Econometrica 87 (5): 1589–1631."
  },
  {
    "objectID": "laczo2015_full_hom.html#estimation-result",
    "href": "laczo2015_full_hom.html#estimation-result",
    "title": "2  Full risk-sharing under homogeneous preferences",
    "section": "2.3 Estimation result",
    "text": "2.3 Estimation result\n\nestimationFullHomTable <- do.call(\n  cbind,\n  map(\n    seq(1, numVillages),\n    ~ c(\n      formatC(estimationResultFullHom[[.]]$parameter, digits = 3, format = \"f\"),\n      str_interp('(${formatC(estimationResultFullHom[[.]]$standardErrors, digits = 3, format = \"f\")})'),\n      formatC(estimationResultFullHom[[.]]$logLikelihood, digits = 3, format = \"f\")\n    )\n  )\n)\ncolnames(estimationFullHomTable) <- c(\"Aurepalle\", \"Kanzara\", \"Shirapur\")\nrownames(estimationFullHomTable) <- c(\n  \"Variance of consumption measurement errors\",\n  \"\",\n  \"Log likelihood\"\n  )\n\nestimationFullHomTable %>% \n  kbl(digits = 3) %>% \n  kable_classic()\n\n\n\n \n  \n      \n    Aurepalle \n    Kanzara \n    Shirapur \n  \n \n\n  \n    Variance of consumption measurement errors \n    0.036 \n    0.037 \n    0.048 \n  \n  \n     \n    (0.004) \n    (0.010) \n    (0.007) \n  \n  \n    Log likelihood \n    -13.808 \n    -20.166 \n    -36.422"
  },
  {
    "objectID": "laczo2015_dataprep.html#sanity-check-compare-against-the-parameters-in-laczo2015",
    "href": "laczo2015_dataprep.html#sanity-check-compare-against-the-parameters-in-laczo2015",
    "title": "1  Data preparation",
    "section": "1.5 Sanity check: compare against the parameters in Laczó (2015)",
    "text": "1.5 Sanity check: compare against the parameters in Laczó (2015)\nJust for a sanity check of my code, I compare the household AR(1) process parameters I derived against the ones provided in the appendix of Laczó (2015). The parameters in the tables below coincide to the parameters in the paper appendix, except the ones for “Low mean, high risk” in Shirapur. I reran the original R script and confirmed that the correct parameters are the ones that I am showing in the table below.\nSince I refactored the origianl R script, I cannot reproduce the village parameters. To be clear, the author has set the random seed in the code, and the reason I cannot reproduce the village parameters is merely because of the difference in code structures.\n\ncreateParameterTableByVillage <- function(\n    village,\n    householdAR1EstimationResult\n) {\n  \n  villageParameters <- array(NA, c(3, 4))\n  colIndex <- 0\n  for (incomeMeanClass in seq(1, 2)) {\n    for (incomeCVClass in seq(1, 2)) {\n      colIndex <- colIndex + 1\n      villageParameters[, colIndex] <- \n        householdAR1EstimationResult[[village]]$AR1ParametersArray[incomeMeanClass, incomeCVClass,]\n    }\n  }\n  rownames(villageParameters) <- c(\n    \"mu\",\n    \"rho\",\n    \"sigmau_squared\"\n  )\n  villageParameters %>% \n    kbl(digits = 3) %>% \n    kable_classic() %>% \n    add_header_above(\n      c(\n        \"\",\n        \"Low mean, \\n low risk\",\n        \"Low mean, \\n high risk\",\n        \"High mean, \\n low risk\",\n        \"High mean, \\n high risk\"\n      )\n    )\n}\ncreateParameterTableByVillage(1, householdAR1EstimationResult)\n\n\n\n\n\nLow mean,  low risk\nLow mean,  high risk\nHigh mean,  low risk\nHigh mean,  high risk\n\n\n  \n    mu \n    182.475 \n    160.201 \n    441.843 \n    391.798 \n  \n  \n    rho \n    0.189 \n    0.626 \n    0.550 \n    0.365 \n  \n  \n    sigmau_squared \n    49.088 \n    68.506 \n    128.741 \n    188.458 \n  \n\n\n\n\ncreateParameterTableByVillage(2, householdAR1EstimationResult)\n\n\n\n\n\nLow mean,  low risk\nLow mean,  high risk\nHigh mean,  low risk\nHigh mean,  high risk\n\n\n  \n    mu \n    235.906 \n    243.872 \n    506.555 \n    525.520 \n  \n  \n    rho \n    0.491 \n    0.285 \n    0.846 \n    0.520 \n  \n  \n    sigmau_squared \n    45.808 \n    79.111 \n    122.302 \n    237.918 \n  \n\n\n\n\ncreateParameterTableByVillage(3, householdAR1EstimationResult)\n\n\n\n\n\nLow mean,  low risk\nLow mean,  high risk\nHigh mean,  low risk\nHigh mean,  high risk\n\n\n  \n    mu \n    263.292 \n    288.625 \n    446.590 \n    642.628 \n  \n  \n    rho \n    0.318 \n    -0.150 \n    0.160 \n    0.199 \n  \n  \n    sigmau_squared \n    78.793 \n    126.987 \n    129.785 \n    271.997"
  },
  {
    "objectID": "laczo2015_lc_hom_estimation.html#estimation-method",
    "href": "laczo2015_lc_hom_estimation.html#estimation-method",
    "title": "4  Risk sharing with limited commitoment: estimation",
    "section": "4.1 Estimation method",
    "text": "4.1 Estimation method\n\n4.1.1 Model\nThe main idea in the estimation is to find the parameters that explain the consumption pattern of a household: \n  c_{it} = \\widehat{c}_{it}(y_t, x_{t - 1}; \\theta),\n where \\widehat{c}_{it} is consumption predicted by the limited commitment risk sharing model with a given set of parameters, \\theta. To explain the discrepancies from the observed consumption and the predicted consumption, I introducemultiplicative measurement errors in consumption, as in the full risk-sharing model. With this, I get the following relationship: \n  \\log \\left(c_{it}^*\\right) = \\log \\left( \\widehat{c}_{it}(y_t, x_{t - 1}; \\theta) \\right) + \\varepsilon_{it}^c,\n where \\varepsilon_{it}^c \\sim N(0, \\gamma_C^2).\nThe previous-period relative Pareto weight, x_{t - 1}, is derived by the ratio of the marginal utilities in the previous period: \n  x_{t - 1} = \\frac{c_{v, t - 1}^{-\\sigma}}{c_{i, t - 1}^{-\\sigma}} = \\frac{c_{v, t - 1}^{*-\\sigma} / \\exp(\\varepsilon_{v, t - 1}^c)}{c_{i, t - 1}^{*-\\sigma} / \\exp(\\varepsilon_{i, t - 1}^c)},\n where \\varepsilon_{vt}^c = \\frac{1}{N} \\sum_i \\varepsilon_{it}^c.\nNote that consumption measurement errors affect the consumption prediction, \\widehat{c} through x_{t - 1}. And the effect is non-linear in the measurement errors and does not have an analytical expression. Therefore, for estimation, I use the simulated maximum likelihood method.\n\n\n4.1.2 Estimation steps\n\nRandomly generate measurement errors, and denote their s’th simulations for i at t by \\varepsilon_{s, i, t}^c.\nCalculate the previous-period relative Pareto weight by x_{s, t - 1} = \\frac{c_{v, t - 1}^{*-\\sigma} / \\exp(\\varepsilon_{s, v, t - 1}^c)}{c_{i, t - 1}^{*-\\sigma} / \\exp(\\varepsilon_{s, i, t - 1}^c)}.\nPredict the consumption given x_{s, t - 1} and y_{s, t}. For this, get x_{s, t} based on x_{s, t - 1} and the bounds in relative Pareto weights given \\theta and y_{s, t}. Note that this updating rule approximates the N-household case by the “one-versus-the-other” approach.\nCalculate the likelihood by f(\\log(c_{it}^*) - \\log \\left( \\widehat{c}_{it}(y_{s, t}, x_{s, t - 1}; \\theta) \\right), \\theta), where f(x, \\theta) is the density function of N(0, \\gamma_C^2).\nTake the average of the likelihood across s: \\frac{1}{S} \\sum_s f(\\log(c_{it}^*) - \\log \\left( \\widehat{c}_{it}(y_{s, t}, x_{s, t - 1}; \\theta) \\right), \\theta).\nTake a logarithm and sum it over i and t.\n\nFor standard errors, I do not assume that the model is correctly specified. Without autocorrelation, the standard errors of the parameters is the squared roots of the diagonal elements of A(\\widehat{\\theta})^{-1} B(\\widehat{\\theta}) A(\\widehat{\\theta})^{-1}. Here, A(\\widehat{\\theta}) is the Hessian matrix of the log-likelihood function and B(\\widehat{\\theta}) is the outer product of scores."
  },
  {
    "objectID": "laczo2015_lc_hom_estimation.html#estimation-code",
    "href": "laczo2015_lc_hom_estimation.html#estimation-code",
    "title": "4  Risk sharing with limited commitoment: estimation",
    "section": "4.2 Estimation code",
    "text": "4.2 Estimation code\n\npacman::p_load(\n  tidyverse,\n  kableExtra,\n  pracma\n)\n\n\n4.2.1 Load data and functions\n\nload('IntermediateData/allData.RData')\nload('IntermediateData/lc_hom_model_functions.RData')\n\n\n\n4.2.2 Simulate random measuremente errors\n\nset.seed(123)\nnumSimulations <- 500\nmeasurementErrorArray <- array(\n  rnorm(hnum * tnum * numSimulations), \n  dim = c(hnum, tnum, numSimulations)\n  )\n\n\n\n4.2.3 Functions to calculate likelihood\n\ninterpolateRelativeParetoWeightBound <- function(\n    householdIncomeGridPoints,\n    villageIncomeGridPoints,\n    relativeParetoWeightBoundsVec,\n    householdIncomeTrue,\n    villageIncomeTrue\n) {\n  interp2(\n    x = householdIncomeGridPoints,\n    y = villageIncomeGridPoints,\n    Z = matrix(\n      relativeParetoWeightBoundsVec,\n      nrow = length(villageIncomeGridPoints),\n      byrow = TRUE\n      ),\n    xp = householdIncomeTrue %>% \n      pmax(min(householdIncomeGridPoints)) %>% \n      pmin(max(householdIncomeGridPoints)),\n    yp = villageIncomeTrue %>% \n      pmax(min(villageIncomeGridPoints)) %>% \n      pmin(max(villageIncomeGridPoints)),\n    method = \"linear\"\n  )\n}\n\nupdateRelativeParetoWeight <- function(\n    previousRelativeParetoWeight,\n    relativeParetoWeightLowerBound,\n    relativeParetoWeightUpperBound\n    ) {\n  previousRelativeParetoWeight %>% \n    pmax(relativeParetoWeightLowerBound) %>% \n    pmin(relativeParetoWeightUpperBound)\n}\n\ncalculateCurrentRelativeParetoWeightEachObsBySimLCHom <- function(\n    village,\n    meanClass,\n    CVClass,\n    householdAR1EstimationResult,\n    villageAR1EstimationResult,\n    householdIncomeTrue,\n    villageIncomeTrue,\n    previousRelativeParetoWeight,\n    relativeParetoWeightBoundsArray\n) {\n  \n    householdIncomeGridPoints <- (\n      householdAR1EstimationResult[[village]]$gridPointsArray[meanClass, CVClass,]\n    )\n    villageIncomeGridPoints <- villageAR1EstimationResult[[village]]$gridPoints\n    \n    relativeParetoWeightLowerBound <- interpolateRelativeParetoWeightBound(\n      householdIncomeGridPoints,\n      villageIncomeGridPoints,\n      relativeParetoWeightBoundsArray[meanClass, CVClass, , 1],\n      householdIncomeTrue,\n      villageIncomeTrue\n      )\n    \n    relativeParetoWeightUpperBound <- interpolateRelativeParetoWeightBound(\n      householdIncomeGridPoints,\n      villageIncomeGridPoints,\n      relativeParetoWeightBoundsArray[meanClass, CVClass, , 2],\n      householdIncomeTrue,\n      villageIncomeTrue\n      )\n    \n    currentRelativeParetoWeight <- updateRelativeParetoWeight(\n      previousRelativeParetoWeight,\n      relativeParetoWeightLowerBound,\n      relativeParetoWeightUpperBound\n      )\n    \n    return(currentRelativeParetoWeight)\n}\n\ncalculateLogLikelihoodForEachObsLCHom <- function(\n    param,\n    village,\n    consdat,\n    incdatRescaled,\n    householdIncMeanClassVec,\n    householdIncCVClassVec,\n    householdAR1EstimationResult,\n    villageAR1EstimationResult,\n    measurementErrorArray,\n    villageIndicatorMatrix,\n    numIncomeStates,\n    numSimulations\n) {\n  \n  delta <- param[1]\n  sigma <- param[2]\n  punishment <- param[3]\n  gammaC2 <- param[4]\n  \n  relativeParetoWeightBoundsArray <- solveLCRiskSharingByVillage(\n      village,\n      delta,\n      sigma,\n      punishment,\n      householdAR1EstimationResult,\n      villageAR1EstimationResult,\n      numIncomeStates,\n      numRelativeParetoWeights = 2000,\n      iterationLimit = 100,\n      diffLimit = 1e-8\n  )\n\n  consVillage <- consdat[villageIndicatorMatrix[, village], ]\n  incdatRescaledVillage <- incdatRescaled[villageIndicatorMatrix[, village], ]\n  measurementErrorArrayVillage <- measurementErrorArray[\n    villageIndicatorMatrix[, village], ,\n    ]\n  numHouseholds <- householdAR1EstimationResult[[village]]$numHouseholds\n  \n  currentRelativeParetoWeightArray <- array(NA, dim = c(numHouseholds, tnum - 1, numSimulations))\n  \n  for (simulationIndex in seq(1, numSimulations)) {\n    \n    consTrue <- (\n      consVillage \n      / exp(measurementErrorArrayVillage[, , simulationIndex] * sqrt(gammaC2))\n      )\n    \n    relativeParetoWeightMatrix <- (\n      outer(\n        rep(1, numHouseholds), \n        calculateMarginalUtility(consTrue, sigma) %>% log %>% colMeans %>% exp\n        )\n      / calculateMarginalUtility(consTrue, sigma)\n    )\n    \n    for (householdIndex in seq(1, numHouseholds)) {\n      for (periodIndex in seq(2, tnum)) {\n        \n        meanClass <- householdIncMeanClassVec[villageIndicatorMatrix[, village]][householdIndex]\n        CVClass <- householdIncCVClassVec[villageIndicatorMatrix[, village]][householdIndex]\n          \n        householdIncomeTrue <- incdatRescaledVillage[householdIndex, periodIndex]\n        villageIncomeTrue <- (incdatRescaledVillage %>% colMeans(na.rm = TRUE))[periodIndex]\n        \n        householdCons <- consVillage[householdIndex, periodIndex]\n        previousRelativeParetoWeight <- relativeParetoWeightMatrix[householdIndex, (periodIndex - 1)]\n        \n        currentRelativeParetoWeightArray[householdIndex, periodIndex - 1, simulationIndex] <- (\n          calculateCurrentRelativeParetoWeightEachObsBySimLCHom(\n            village,\n            meanClass,\n            CVClass,\n            householdAR1EstimationResult,\n            villageAR1EstimationResult,\n            householdIncomeTrue,\n            villageIncomeTrue,\n            previousRelativeParetoWeight,\n            relativeParetoWeightBoundsArray\n            )\n        )\n      }\n    }\n  }\n  \n  likelihoodArrayPerSim <- array(NA, dim = c(numHouseholds, tnum - 1, numSimulations))\n  for (simulationIndex in seq(1, numSimulations)) {\n    \n    for (householdIndex in seq(1, numHouseholds)) {\n      for (periodIndex in seq(2, tnum)) {\n        logConsPredict <- (\n          log(\n            (incdatRescaledVillage[, periodIndex] %>% sum(na.rm = TRUE)) \n            / (\n              (currentRelativeParetoWeightArray[, periodIndex - 1, simulationIndex]^(1 / sigma)) %>% \n                sum(na.rm = TRUE)\n              )\n            )\n          + 1 / sigma * log(\n            currentRelativeParetoWeightArray[householdIndex, periodIndex - 1, simulationIndex]\n            )\n        )\n        likelihoodArrayPerSim[householdIndex, periodIndex - 1, simulationIndex] <- dnorm(\n          log(consVillage[householdIndex, periodIndex]) \n          - logConsPredict,\n          sd = sqrt(gammaC2)\n        )\n      }\n    }\n  }\n  \n  logLikelihoodArray <- array(NA, dim = c(numHouseholds, tnum - 1))\n  for (householdIndex in seq(1, numHouseholds)) {\n    for (periodIndex in seq(2, tnum)) {\n      likelihoodSimMean <- likelihoodArrayPerSim[householdIndex, periodIndex - 1, ] %>% mean(na.rm = TRUE)\n      logLikelihoodArray[householdIndex, periodIndex - 1] <- log(likelihoodSimMean %>% pmax(1e-8))\n    }\n  }\n  \n  return(logLikelihoodArray)\n}\n\ncalculateLogLikelihoodLCHom <- function(\n    param,\n    village,\n    consdat,\n    incdatRescaled,\n    householdIncMeanClassVec,\n    householdIncCVClassVec,\n    householdAR1EstimationResult,\n    villageAR1EstimationResult,\n    measurementErrorArray,\n    villageIndicatorMatrix,\n    numIncomeStates,\n    numSimulations\n    ) {\n\n  logLikelihoodForEachObs <- calculateLogLikelihoodForEachObsLCHom(\n    param,\n    village,\n    consdat,\n    incdatRescaled,\n    householdIncMeanClassVec,\n    householdIncCVClassVec,\n    householdAR1EstimationResult,\n    villageAR1EstimationResult,\n    measurementErrorArray,\n    villageIndicatorMatrix,\n    numIncomeStates,\n    numSimulations\n  )\n  \n  logLikelihood <- sum(logLikelihoodForEachObs)\n  \n  return(- logLikelihood)\n}\n\n\nestimateMLLCHom <- function(\n  initialParam,\n  lowerBounds,\n  upperBounds,\n  village,\n  consdat,\n  incdatRescaled,\n  householdIncMeanClassVec,\n  householdIncCVClassVec,\n  householdAR1EstimationResult,\n  villageAR1EstimationResult,\n  measurementErrorArray,\n  villageIndicatorMatrix,\n  numIncomeStates,\n  numSimulations\n) {\n  \n  optimRes <- optim(\n    initialParam,\n    calculateLogLikelihoodLCHom,\n    village = village,\n    consdat = consdat,\n    incdatRescaled = incdatRescaled,\n    householdIncMeanClassVec = householdIncMeanClassVec,\n    householdIncCVClassVec = householdIncCVClassVec,\n    householdAR1EstimationResult = householdAR1EstimationResult,\n    villageAR1EstimationResult = villageAR1EstimationResult,\n    measurementErrorArray = measurementErrorArray,\n    villageIndicatorMatrix = villageIndicatorMatrix,\n    numIncomeStates = numIncomeStates,\n    numSimulations = numSimulations,\n    method = \"L-BFGS-B\",\n    lower = lowerBounds,\n    upper = upperBounds,\n    control = list(trace = 5, maxit = 200),\n    hessian = TRUE\n    )\n  \n  \n  score <- jacobian(\n    function(x) calculateLogLikelihoodForEachObsLCHom(\n      x,\n      village,\n      consdat,\n      incdatRescaled,\n      householdIncMeanClassVec,\n      householdIncCVClassVec,\n      householdAR1EstimationResult,\n      villageAR1EstimationResult,\n      measurementErrorArray,\n      villageIndicatorMatrix,\n      numIncomeStates,\n      numSimulations\n    ) %>% as.vector,\n    optimRes$par\n  )\n  \n  standardErrors <- (\n    (optimRes$hessian %>% solve) \n    %*% reduce(\n      map(\n        seq(nrow(score)),\n        ~ outer(score[., ], score[., ])\n      ),\n      function(x, y) x + y\n    )\n    %*% (optimRes$hessian %>% solve) \n  ) %>% diag %>% sqrt\n  \n  return(list(optimRes = optimRes, standardErrors = standardErrors))\n}"
  },
  {
    "objectID": "laczo2015_lc_hom_estimation.html#estimation",
    "href": "laczo2015_lc_hom_estimation.html#estimation",
    "title": "4  Risk sharing with limited commitoment: estimation",
    "section": "4.3 Estimation",
    "text": "4.3 Estimation\n\ninitialParam <- c(0.95, 3.0, 0.3, 0.03)\nlowerBounds <- c(0.5, 1.0, 0.0, 1e-3)\nupperBounds <- c(0.98, 5.0, 0.99, 1.0)\n\nestimationLCHomRes <- map(\n  seq(1, numVillages),\n  ~ estimateMLLCHom(\n    initialParam,\n    lowerBounds,\n    upperBounds,\n    .,\n    consdat,\n    incdatRescaled,\n    householdIncMeanClassVec,\n    householdIncCVClassVec,\n    householdAR1EstimationResult,\n    villageAR1EstimationResult,\n    measurementErrorArray,\n    villageIndicatorMatrix,\n    numIncomeStates,\n    numSimulations\n  )\n)\n\n\n\n\n\n\n\n\ncreateRegressionTable <- function(village) {\n  \n  estimationLCHomTable <- c()\n  for (varIndex in seq_along(estimationLCHomRes[[village]]$optimRes$par)) {\n    estimationLCHomTable <- c(\n      estimationLCHomTable,\n      formatC(estimationLCHomRes[[village]]$optimRes$par[varIndex], digits = 3, format = \"f\"),\n      str_interp(\n        '(${formatC(estimationLCHomRes[[village]]$standardErrors[varIndex], digits = 3, format = \"f\")})'\n        )\n      )\n  }\n  estimationLCHomTable <- c(\n    estimationLCHomTable,\n    formatC(-estimationLCHomRes[[village]]$optimRes$value, digits = 3, format = \"f\")\n  )\n  return(estimationLCHomTable)\n}\n\nestimationLCHomTable <- do.call(\n  cbind,\n  map(\n    seq(1, numVillages),\n    createRegressionTable\n  )\n)\n\ncolnames(estimationLCHomTable) <- c(\"Aurepalle\", \"Kanzara\", \"Shirapur\")\nrownames(estimationLCHomTable) <- c(\n  \"Discount factor\",\n  \"\",\n  \"Coef of RRA\",\n  \"\",\n  \"punishment parameter\",\n  \"\",\n  \"Variance of consumption measurement errors\",\n  \"\",\n  \"Log likelihood\"\n  )\n\nestimationLCHomTable %>% \n  kbl(digits = 3) %>% \n  kable_classic()\n\n\n\n \n  \n      \n    Aurepalle \n    Kanzara \n    Shirapur \n  \n \n\n  \n    Discount factor \n    0.980 \n    0.748 \n    0.661 \n  \n  \n     \n    (0.025) \n    (0.030) \n    (0.232) \n  \n  \n    Coef of RRA \n    2.998 \n    2.905 \n    2.756 \n  \n  \n     \n    (0.078) \n    (0.013) \n    (0.066) \n  \n  \n    punishment parameter \n    0.234 \n    0.347 \n    0.004 \n  \n  \n     \n    (0.035) \n    (0.032) \n    (0.004) \n  \n  \n    Variance of consumption measurement errors \n    0.035 \n    0.028 \n    0.059 \n  \n  \n     \n    (0.005) \n    (0.004) \n    (0.009) \n  \n  \n    Log likelihood \n    -3.129 \n    -9.478 \n    -18.022"
  },
  {
    "objectID": "laczo2015_lc_hom_estimation.html#estimation-result",
    "href": "laczo2015_lc_hom_estimation.html#estimation-result",
    "title": "4  Risk sharing with limited commitoment: estimation",
    "section": "4.3 Estimation result",
    "text": "4.3 Estimation result\n\ninitialParam <- c(0.95, 3.0, 0.3, 0.03)\nlowerBounds <- c(0.5, 1.0, 0.0, 1e-3)\nupperBounds <- c(0.98, 5.0, 0.99, 1.0)\n\nestimationLCHomRes <- map(\n  seq(1, numVillages),\n  ~ estimateMLLCHom(\n    initialParam,\n    lowerBounds,\n    upperBounds,\n    .,\n    consdat,\n    incdatRescaled,\n    householdIncMeanClassVec,\n    householdIncCVClassVec,\n    householdAR1EstimationResult,\n    villageAR1EstimationResult,\n    measurementErrorArray,\n    villageIndicatorMatrix,\n    numIncomeStates,\n    numSimulations\n  )\n)\n\n\n\n\n\n\n\n\ncreateRegressionTable <- function(village) {\n  \n  estimationLCHomTable <- c()\n  for (varIndex in seq_along(estimationLCHomRes[[village]]$optimRes$par)) {\n    estimationLCHomTable <- c(\n      estimationLCHomTable,\n      formatC(estimationLCHomRes[[village]]$optimRes$par[varIndex], digits = 3, format = \"f\"),\n      str_interp(\n        '(${formatC(estimationLCHomRes[[village]]$standardErrors[varIndex], digits = 3, format = \"f\")})'\n        )\n      )\n  }\n  estimationLCHomTable <- c(\n    estimationLCHomTable,\n    formatC(-estimationLCHomRes[[village]]$optimRes$value, digits = 3, format = \"f\")\n  )\n  return(estimationLCHomTable)\n}\n\nestimationLCHomTable <- do.call(\n  cbind,\n  map(\n    seq(1, numVillages),\n    createRegressionTable\n  )\n)\n\ncolnames(estimationLCHomTable) <- c(\"Aurepalle\", \"Kanzara\", \"Shirapur\")\nrownames(estimationLCHomTable) <- c(\n  \"Discount factor\",\n  \"\",\n  \"Coef of RRA\",\n  \"\",\n  \"punishment parameter\",\n  \"\",\n  \"Variance of consumption measurement errors\",\n  \"\",\n  \"Log likelihood\"\n  )\n\nestimationLCHomTable %>% \n  kbl(digits = 3) %>% \n  kable_classic()\n\n\n\n \n  \n      \n    Aurepalle \n    Kanzara \n    Shirapur \n  \n \n\n  \n    Discount factor \n    0.980 \n    0.748 \n    0.661 \n  \n  \n     \n    (0.025) \n    (0.030) \n    (0.232) \n  \n  \n    Coef of RRA \n    2.998 \n    2.905 \n    2.756 \n  \n  \n     \n    (0.078) \n    (0.013) \n    (0.066) \n  \n  \n    punishment parameter \n    0.234 \n    0.347 \n    0.004 \n  \n  \n     \n    (0.035) \n    (0.032) \n    (0.004) \n  \n  \n    Variance of consumption measurement errors \n    0.035 \n    0.028 \n    0.059 \n  \n  \n     \n    (0.005) \n    (0.004) \n    (0.009) \n  \n  \n    Log likelihood \n    -3.129 \n    -9.478 \n    -18.022"
  }
]