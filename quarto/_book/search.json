[
  {
    "objectID": "laczo2015_dataprep.html#load-packages",
    "href": "laczo2015_dataprep.html#load-packages",
    "title": "1  Data preparation",
    "section": "1.1 Load packages",
    "text": "1.1 Load packages\n\npacman::p_load(\n  tidyverse,\n  kableExtra\n)"
  },
  {
    "objectID": "laczo2015_dataprep.html#load-datasets",
    "href": "laczo2015_dataprep.html#load-datasets",
    "title": "1  Data preparation",
    "section": "1.3 Load datasets",
    "text": "1.3 Load datasets\n\nload('Laczo2015/allest')\n\n\n1.3.1 Basic data processes of raw data\n\nnumVillages <- max(villagedat)\nvillageIndicatorMatrix <- do.call(\n  cbind,\n  map(\n    seq(1, numVillages),\n    ~ villagedat[, 1] == .\n  )\n)\n\n\ncreateVillageAggregateByYear <- function(\n    data, \n    func, \n    .numVillages = numVillages, \n    .villageIndicatorMatrix = villageIndicatorMatrix\n    ) {\n  do.call(\n    rbind, \n    map(\n      seq(1, .numVillages),\n      ~ data[.villageIndicatorMatrix[, .],] %>% func\n      )\n    )\n  }\n\ncreateVillageAggregate <- function(\n    data, \n    func, \n    .numVillages = numVillages, \n    .villageIndicatorMatrix = villageIndicatorMatrix\n    ) {\n    map_vec(\n      seq(1, .numVillages),\n      ~ data[.villageIndicatorMatrix[, .],] %>% func\n    )\n  }\n\n\nvilMeanIncByYear <- createVillageAggregateByYear(incdat, colMeans)\nvilSumIncByYear <- createVillageAggregateByYear(incdat, colSums)\nvilMeanInc <- createVillageAggregate(incdat, mean)\nvilMeanConsByYear <- createVillageAggregateByYear(consdat, colMeans)\nvilMeanLogCons <- createVillageAggregateByYear(log(consdat), colMeans)\nvilSumConsByYear <- createVillageAggregateByYear(consdat, colSums)\nvilMeanCons <- createVillageAggregate(consdat, mean)\n\n\n\n\n\nincLow <- createVillageAggregate(incdat, function(x) quantile(x, 0.025, na.rm = TRUE))\nincHigh <- createVillageAggregate(incdat, function(x) quantile(x, 0.975, na.rm = TRUE))\nconsLow <- createVillageAggregate(consdat, function(x) quantile(x, 0.025, na.rm = TRUE))\nconsHigh <- createVillageAggregate(consdat, function(x) quantile(x, 0.975, na.rm = TRUE))\n\nSince there is no saving in the model, the consumption and income should coincide. For this, I rescale the income so that the means of consumption and income are identical in each village.\n\nvilConsPerIncByYear <- vilMeanConsByYear / vilMeanIncByYear\nincdatRescaled <- incdat * (villageIndicatorMatrix %*% vilConsPerIncByYear)\nvilMeanIncByYearRescaled <- createVillageAggregateByYear(incdatRescaled, colMeans)"
  },
  {
    "objectID": "laczo2015_dataprep.html#estimate-income-processes",
    "href": "laczo2015_dataprep.html#estimate-income-processes",
    "title": "1  Data preparation",
    "section": "1.4 Estimate income processes",
    "text": "1.4 Estimate income processes\nThe main step of this data processing is to estimate the income process, first of households and second of the “village” (= average of households in a village). We want the income process of the village since, in estimation, we consider the risk-sharing transfers between a household and the village so that we can consider one-to-one transfers instead of on-to-n transfers.\nFor this, first I estimate the income process of households by types, where the types are by income’s mean and coefficient of variance (CV). Here, in total we have 4 types (high/low mean income X high/low CV income). If there were long enough income data, we would have been able to estimate the income process of each household, but given the 6 observations per household for income in ICRISAT data, the author took this approach. Also, to mitigate the effect of outliers, income smaller than 2.5 percentile or larger than 97.5 percentile in each village is not used for estimation.\nThis estimation process is detailed in the paper’s online appendix\n\nnumIncomeStatesHH <- 8\nnumIncomeStatesVillage <- 5\n\n\n1.4.1 Household\n\nhouseholdIncMean <- incdatRescaled %>% rowMeans\nhouseholdIncSD <- apply(incdatRescaled, 1, sd, na.rm = TRUE)\nhouseholdIncCV <- householdIncSD / householdIncMean\n\nvilIncMeanMedian <- createVillageAggregate(\n  as.matrix(householdIncMean), \n  median\n  )\nvilIncCVMedian <- createVillageAggregate(\n  as.matrix(householdIncCV), \n  median\n  )\n\n\n1.4.1.1 Estimate AR(1) process\nFirst I estimate the AR(1) process of households’ income. In particular, the following AR(1) process is estimated:\n\n  y_{it} = (1 - \\rho) \\mu + \\rho y_{i, t - 1} + u_{it},\n\nwhere y_{it} is the income of a household i in period t.\nThe parameters are given as\n\n  \\mu = E(y_{it}) \\\\\n  \\rho = Cor(y_{it}, y_{i, t - 1}) \\\\\n  \\sigma_{u}^2 = (1 - \\rho^2) Var(y_{it}).\n\n\nhouseholdIncMeanClassVec <- (householdIncMean > (villageIndicatorMatrix %*% vilIncMeanMedian)) + 1\nhouseholdIncCVClassVec <- (householdIncCV > (villageIndicatorMatrix %*% vilIncCVMedian)) + 1\n\nlaggedIncdatRescaled <- cbind(NA, incdatRescaled[, seq(1, tnum - 1)])\nincdatRescaled[\n  (incdat <= as.vector(villageIndicatorMatrix %*% incLow)) |\n    (incdat >= as.vector(villageIndicatorMatrix %*% incHigh))] <- NA\nlaggedIncdatRescaled[\n  (incdat <= as.vector(villageIndicatorMatrix %*% incLow)) |\n    (incdat >= as.vector(villageIndicatorMatrix %*% incHigh))] <- NA\n\ngetDataByMeanCVClassByVillage <- function(\n    village, \n    data, \n    meanClass, \n    CVClass,\n    meanClassVec = householdIncMeanClassVec,\n    CVClassVec = householdIncCVClassVec,\n    .villageIndicatorMatrix = villageIndicatorMatrix\n    ) {\n  data[\n    (meanClassVec == meanClass) & \n      (CVClassVec == CVClass) &\n      (.villageIndicatorMatrix[, village])\n  ]\n}\n\ncalculateAR1Parameters <- function(data, laggedData) {\n  mu <- mean(data, na.rm = TRUE)\n  rho <- cor(\n    data,\n    laggedData,\n    use=\"complete.obs\"\n    )\n  sigmau <- sqrt(var(data, na.rm = TRUE) * (1 - rho^2))\n  \n  return(list(mu = mu, rho = rho, sigmau = sigmau))\n}\n\n\n\n1.4.1.2 Approximate the AR(1) process with Markov chain for income of households\nGiven the estimated parameters, I approximate the AR(1) process with discretization. For this, the author used Tauchen’s method with a small modification that, instead of assigning the bounds of grid points with a parameter, the quantiles of income distributions are used to determine each grid point. To guarantee that the mean income at the steady state of the estimated process coincides with the actual mean income, the grid points are rescaled at the end.\n\ncalculateGridPoints <- function(numStates, data) {\n  gridQuantile <- seq(0, 1, by = 1 / numStates)\n  map_dbl(\n    (gridQuantile[1:(length(gridQuantile) - 1)] + gridQuantile[2:length(gridQuantile)]) / 2,\n    ~ quantile(data, ., na.rm = TRUE)\n  )\n}\n\napproximateAR1Tauchen <- function(numStates, data, mu, rho, sigma) {\n  \n  gridPoints <- calculateGridPoints(numStates, data)\n  \n  transitionMatrix <- array(NA, c(numStates, numStates))\n  for (currentState in 1:numStates) {\n    transitionMatrix[currentState, 1] <- (\n      pnorm(\n        ((gridPoints[2] + gridPoints[1]) / 2 \n         - (1 - rho) * mu - rho * gridPoints[currentState])\n        / sigma\n        )\n      )\n    transitionMatrix[currentState, numStates] <- 1 - pnorm(\n      ((gridPoints[numStates] + gridPoints[numStates - 1]) / 2 \n       - (1 - rho) * mu - rho * gridPoints[currentState])\n      / sigma\n      )\n    }\n  for (currentState in 1:numStates) {\n    for (nextState in 2:(numStates - 1)) {\n        transitionMatrix[currentState, nextState] <- (\n          pnorm(\n          ((gridPoints[nextState + 1] + gridPoints[nextState]) / 2 \n           - (1 - rho) * mu - rho * gridPoints[currentState])\n          / sigma\n          )\n        - pnorm(\n          ((gridPoints[nextState] + gridPoints[nextState - 1]) / 2 \n           - (1 - rho) * mu - rho * gridPoints[currentState])\n          / sigma\n          )\n          )\n        }\n    }\n  return(list(transitionMatrix = transitionMatrix, gridPoints = gridPoints))\n}\n\ncalculateSteadyStateProb <- function(transitionMatrix) {\n  (\n    eigen(t(transitionMatrix))$vector[, 1]\n    / sum(eigen(t(transitionMatrix))$vector[, 1])\n  )\n}\n\nrescaleGridPoints <- function(transitionMatrix, gridPoints, data) {\n  steadyStateProb <- calculateSteadyStateProb(transitionMatrix)\n  rescaleScalar <- as.numeric(\n    mean(data, na.rm = TRUE) / gridPoints \n    %*% steadyStateProb\n    )\n  gridPointsRescaled <- gridPoints * rescaleScalar\n  return(list(gridPointsRescaled = gridPointsRescaled, steadyStateProb = steadyStateProb))\n}\n\napproximateAR1TauchenWithRescaling <- function(numStates, data, mu, rho, sigma) {\n  \n  TauchenResult <- approximateAR1Tauchen(numStates, data, mu, rho, sigma)\n  transitionMatrix <- TauchenResult$transitionMatrix\n  gridPoints <- TauchenResult$gridPoints\n  gridPointsRescaledResult <- rescaleGridPoints(\n    transitionMatrix, gridPoints, data\n    )\n  gridPointsRescaled <- gridPointsRescaledResult$gridPointsRescaled\n  steadyStateProb <- gridPointsRescaledResult$steadyStateProb\n  \n  return(list(\n    transitionMatrix = transitionMatrix, \n    gridPointsRescaled = gridPointsRescaled,\n    steadyStateProb = steadyStateProb\n    ))\n}\n\nestimateHouseholdIncomeTransitionProcessByVillage <- function(\n    village,\n    numStates,\n    data,\n    laggedData\n) {\n  gridPointsArray <- array(NA, c(2, 2, numStates))\n  transitionMatrixArray <- array(NA, c(2, 2, numStates, numStates))\n  steadyStateProbArray <- array(NA, c(2, 2, numStates)) \n  AR1ParametersArray <- array(NA, c(2, 2, 3))\n  \n  for (incomeMeanClass in seq(1, 2)) {\n    for (incomeCVClass in seq(1, 2)) {\n      incdatRescaledMeanCVClass <- getDataByMeanCVClassByVillage(\n        village, data, incomeMeanClass, incomeCVClass\n        )\n      laggeedIncdatRescaledMeanCVClass <- getDataByMeanCVClassByVillage(\n        village, laggedData, incomeMeanClass, incomeCVClass\n        )\n      AR1Parameters <- calculateAR1Parameters(incdatRescaledMeanCVClass, laggeedIncdatRescaledMeanCVClass)\n      TauchenResult <- approximateAR1TauchenWithRescaling(\n        numIncomeStatesHH, incdatRescaledMeanCVClass, \n        AR1Parameters$mu, AR1Parameters$rho, AR1Parameters$sigmau\n        )\n      gridPointsArray[incomeMeanClass, incomeCVClass,] <- TauchenResult$gridPoints\n      transitionMatrixArray[incomeMeanClass, incomeCVClass,,] <- TauchenResult$transitionMatrix\n      steadyStateProbArray[incomeMeanClass, incomeCVClass,] <- TauchenResult$steadyStateProb\n      AR1ParametersArray[incomeMeanClass, incomeCVClass,] <- unlist(AR1Parameters)\n    }\n  }\n  \n  return(list(\n    gridPointsArray = gridPointsArray,\n    transitionMatrixArray = transitionMatrixArray,\n    steadyStateProbArray = steadyStateProbArray,\n    AR1ParametersArray = AR1ParametersArray\n    ))\n}\n  \nhouseholdAR1EstimationResult <- map(\n  seq(1, numVillages),\n  ~ estimateHouseholdIncomeTransitionProcessByVillage(., numIncomeStatesHH, incdatRescaled, laggedIncdatRescaled)\n)\n\n\n\n\n\n\n\n1.4.2 Village\nGiven the estimated household income processes, I estimate the income process of the “village”. For this, I first simulate the average village income, using the parameters estimated above. In particular, I simulate the household income over 1000 periods, then compute the mean income in each period. After excluding the first 100 observations to use the income at the steady state, I estimate the parameters using the same method as for the household income.\n\n1.4.2.1 Simulate income process for village\n\nnumVillageIncomeSimulations <- 1000\nnumVillageIncomeSimulationsPeriodDrop <- 100\n\n\nsample1stPeriodIncomeStateByMeanCVClass <- function(\n    meanClass, \n    CVClass, \n    numStates, \n    steadyStateProbArray\n    ) {\n  sample(\n    seq(1, numStates),\n    1,\n    prob = steadyStateProbArray[meanClass, CVClass, ]\n    )\n}\n\nsampleConditionalIncomeStateByMeanCVClass <- function(\n    meanClass, \n    CVClass,\n    numStates,\n    transitionMatrixArray,\n    previousState\n) {\n  sample(\n    seq(1, numStates),\n    1,\n    prob = transitionMatrixArray[meanClass, CVClass, previousState,]\n  )\n}\n\nsampleAllIncomeStateByMeanCVClass <- function(\n    meanClass, \n    CVClass,\n    numStates,\n    steadyStateProbArray,\n    transitionMatrixArray,\n    numSimulation = numVillageIncomeSimulations\n) {\n  incomeStateVector <- vector(mode = \"integer\", length = numSimulation)\n  incomeStateVector[1] <- sample1stPeriodIncomeStateByMeanCVClass(\n    meanClass, \n    CVClass, \n    numStates, \n    steadyStateProbArray\n    )\n  for (period in seq(2, numSimulation)) {\n    incomeStateVector[period] <- sampleConditionalIncomeStateByMeanCVClass(\n      meanClass, \n      CVClass,\n      numStates,\n      transitionMatrixArray,\n      incomeStateVector[period - 1]\n      )\n  }\n  return(incomeStateVector)\n}\n\nsimulateHouseholdIncomeByMeanCVClass <- function(\n    meanClass, \n    CVClass,\n    numStates,\n    steadyStateProbArray,\n    transitionMatrixArray,\n    gridPointsArray,\n    numSimulation = numVillageIncomeSimulations\n) {\n  incomeStateVec <- sampleAllIncomeStateByMeanCVClass(\n    meanClass, \n    CVClass,\n    numStates,\n    steadyStateProbArray,\n    transitionMatrixArray)\n  gridPointsArray[meanClass, CVClass, incomeStateVec]\n}\n\nsimulateHouseholdIncomeByVillage <- function(\n    village,\n    meanClassVec,\n    CVClassVec,\n    numStates,\n    householdAR1EstimationResult,\n    .villageIndicatorMatrix = villageIndicatorMatrix\n) {\n  \n  meanClassVillage <- meanClassVec[villageIndicatorMatrix[, village]]\n  CVClassVillage <- CVClassVec[villageIndicatorMatrix[, village]]\n  steadyStateProbArrayVillage <- householdAR1EstimationResult[[village]]$steadyStateProbArray\n  transitionMatrixArrayVillage <- householdAR1EstimationResult[[village]]$transitionMatrixArray\n  gridPointsArrayVillage <- householdAR1EstimationResult[[village]]$gridPointsArray\n  \n  do.call(\n    rbind,\n    map2(\n      meanClassVillage, CVClassVillage,\n      ~ simulateHouseholdIncomeByMeanCVClass(\n        .x, .y, numStates, steadyStateProbArrayVillage, transitionMatrixArrayVillage, gridPointsArrayVillage\n        )\n    )\n  )\n  \n}\n\n\n\n1.4.2.2 Estimate the income process of the village\nGiven the simulated income data, I estimate the income process parameters and apprximate the process with Tauchen’s method. Although the steps are identical to the ones I took above, it should be noted that the village income, y_{vt} is calculated so that:\n\n  \\log (y_{vt}) = \\frac{1}{N} \\sum_{i} \\log(y_{it}),\n instead of \n  y_{vt} = \\frac{1}{N} \\sum_{i} y_{it},\n where y_{it} is income of a household i and N is the number of households in a village. This is because the first form is better fit to the theoretical model.\n\nestimateVillagencomeTransitionProcessByVillage <- function(\n    village,\n    meanClassVec,\n    CVClassVec,\n    numStatesHH,\n    numStatesVillage,\n    householdAR1EstimationResult,\n    .villageIndicatorMatrix = villageIndicatorMatrix,\n    .numVillageIncomeSimulationsPeriodDrop = numVillageIncomeSimulationsPeriodDrop,\n    .numVillageIncomeSimulations = numVillageIncomeSimulations\n){\n  householdIncomeSimulationResult <- simulateHouseholdIncomeByVillage(\n    village,\n    meanClassVec,\n    CVClassVec,\n    numStatesHH,\n    householdAR1EstimationResult\n  )\n  \n  villageSimulatedIncMean <- colMeans(\n    householdIncomeSimulationResult[\n      , .numVillageIncomeSimulationsPeriodDrop:.numVillageIncomeSimulations\n      ]\n  )\n  villageSimulatedIncLogMean <- villageSimulatedIncMean %>% log\n  villageSimulatedLaggedIncLogMean <- colMeans(\n    householdIncomeSimulationResult[\n      , (.numVillageIncomeSimulationsPeriodDrop - 1):(.numVillageIncomeSimulations - 1)\n      ]\n  ) %>% log\n  \n  villageAR1Parameters <- calculateAR1Parameters(\n    villageSimulatedIncLogMean,\n    villageSimulatedLaggedIncLogMean \n  )\n  \n  villageAR1TauchenApproximation <- approximateAR1Tauchen(\n    numStatesVillage, villageSimulatedIncLogMean, \n    villageAR1Parameters$mu, villageAR1Parameters$rho, villageAR1Parameters$sigmau\n  )\n  \n  villageLogIncomeGridPoints <- calculateGridPoints(numStatesVillage, villageSimulatedIncLogMean)\n  \n  villageIncomeGridPointsRescaled <- rescaleGridPoints(\n      villageAR1TauchenApproximation$transitionMatrix, \n      exp(villageLogIncomeGridPoints),\n      villageSimulatedIncMean\n      )\n  \n  return(list(\n    transitionMatrix = villageAR1TauchenApproximation$transitionMatrix,\n    gridPoints = villageIncomeGridPointsRescaled$gridPointsRescaled\n  ))\n}\n\nvillageAR1EstimationResult <- map(\n  seq(1, numVillages),\n  ~ estimateVillagencomeTransitionProcessByVillage(\n      .,\n      householdIncMeanClassVec,\n      householdIncCVClassVec,\n      numIncomeStatesHH,\n      numIncomeStatesVillage,\n      householdAR1EstimationResult\n  )\n)"
  },
  {
    "objectID": "laczo2015_dataprep.html#sanity-check-compare-against-the-parameters-in-the-original-paper",
    "href": "laczo2015_dataprep.html#sanity-check-compare-against-the-parameters-in-the-original-paper",
    "title": "1  Data preparation",
    "section": "1.5 Sanity check: compare against the parameters in the original paper",
    "text": "1.5 Sanity check: compare against the parameters in the original paper\nJust for a sanity check of my code, I compare the household AR(1) process parameters I derived against the ones provided in the appendix of the paper. The parameters in the tables below coincide to the parameters in the paper appendix, excpet the ones for “Low mean, high risk” in Shirapur. I reran the original R script and confirmed that the correct parameters are the ones that I am showing in the table below.\nSince I refactored the origianl R script, I cannot reproduce the village parameters. To be clear, the author has set the random seed in the code, and the reason I cannot reproduce the village parameters is merely because of the difference in code structures.\n\ncreateParameterTableByVillage <- function(\n    village,\n    householdAR1EstimationResult\n) {\n  \n  villageParameters <- array(NA, c(3, 4))\n  colIndex <- 0\n  for (incomeMeanClass in seq(1, 2)) {\n    for (incomeCVClass in seq(1, 2)) {\n      colIndex <- colIndex + 1\n      villageParameters[, colIndex] <- \n        householdAR1EstimationResult[[village]]$AR1ParametersArray[incomeMeanClass, incomeCVClass,]\n    }\n  }\n  rownames(villageParameters) <- c(\n    \"mu\",\n    \"rho\",\n    \"sigmau_squared\"\n  )\n  villageParameters %>% \n    kbl(digits = 3) %>% \n    kable_classic() %>% \n    add_header_above(\n      c(\n        \"\",\n        \"Low mean, \\n low risk\",\n        \"Low mean, \\n high risk\",\n        \"High mean, \\n low risk\",\n        \"High mean, \\n high risk\"\n      )\n    )\n}\ncreateParameterTableByVillage(1, householdAR1EstimationResult)\n\n\n\n\n\nLow mean,  low risk\nLow mean,  high risk\nHigh mean,  low risk\nHigh mean,  high risk\n\n\n  \n    mu \n    182.475 \n    160.201 \n    441.843 \n    391.798 \n  \n  \n    rho \n    0.189 \n    0.626 \n    0.550 \n    0.365 \n  \n  \n    sigmau_squared \n    49.088 \n    68.506 \n    128.741 \n    188.458 \n  \n\n\n\n\ncreateParameterTableByVillage(2, householdAR1EstimationResult)\n\n\n\n\n\nLow mean,  low risk\nLow mean,  high risk\nHigh mean,  low risk\nHigh mean,  high risk\n\n\n  \n    mu \n    235.906 \n    243.872 \n    506.555 \n    525.520 \n  \n  \n    rho \n    0.491 \n    0.285 \n    0.846 \n    0.520 \n  \n  \n    sigmau_squared \n    45.808 \n    79.111 \n    122.302 \n    237.918 \n  \n\n\n\n\ncreateParameterTableByVillage(3, householdAR1EstimationResult)\n\n\n\n\n\nLow mean,  low risk\nLow mean,  high risk\nHigh mean,  low risk\nHigh mean,  high risk\n\n\n  \n    mu \n    263.292 \n    288.625 \n    446.590 \n    642.628 \n  \n  \n    rho \n    0.318 \n    -0.150 \n    0.160 \n    0.199 \n  \n  \n    sigmau_squared \n    78.793 \n    126.987 \n    129.785 \n    271.997"
  },
  {
    "objectID": "laczo2015_dataprep.html",
    "href": "laczo2015_dataprep.html",
    "title": "1  My document",
    "section": "",
    "text": "2 Laczo (2015) R scripts for data preparation"
  },
  {
    "objectID": "laczo2015_dataprep.html#global-settings",
    "href": "laczo2015_dataprep.html#global-settings",
    "title": "1  Data preparation",
    "section": "1.2 Global settings",
    "text": "1.2 Global settings\n\nset.seed(123)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estimation of risk-sharing model with limited commitment",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2\n\n2 + 2\n\n[1] 4"
  },
  {
    "objectID": "laczo2015_full_hom.html",
    "href": "laczo2015_full_hom.html",
    "title": "2  Full risk-sharing under homogeneous preferences",
    "section": "",
    "text": "3 Full risk-sharing with homogeneous preferences"
  },
  {
    "objectID": "laczo2015_full_hom.html#theory",
    "href": "laczo2015_full_hom.html#theory",
    "title": "2  Full risk-sharing under homogeneous preferences",
    "section": "3.1 Theory",
    "text": "3.1 Theory\nTo derive the likelihood function, I review the theoretical result.\n\n3.1.1 Settings\nSuuposed that there are N households in a village. Each household maximized its expected lifetime utility: \n  E \\left[ \\sum_{t = 1}^{\\infty} \\delta^t u (c_{it}) \\right],\n where \\delta is the discount factor, u is instantaneous preference of households (note that here I assume homogeneous preference), and c_{it} is consumption by household i at time t. Assume that there is no saving.\nFor income, let s^t = (s_1, \\dots, s_t) the history of income states from time 1 to t, and let y_{it}(s_t) be household i’s income at time t and state s_t. Assume that the income process is a Markov process and is independent across households.\nWith these, I find the Pareto-optimal allocations. For this, a weighted sum of households’ expected lifetime utilities is maximized. In other words, a social planner solves the following maximization problem:\n\n  \\max_{\\{c_{it}(s^t)\\}} \\quad \\sum_i \\lambda_i \\sum_{t = 1}^{\\infty} \\sum_{s^t} \\delta^t \\pi(s^t) u(c_{it}(s^t))\n subject to the resource constraint \n  \\sum_i c_{it}(s^t) \\le \\sum_i y_{it}(s_t) \\quad \\forall s^t, \\forall t,\n where \\lambda_i is the Pareto weight of i, \\pi(s^t) is the probability that the history s^t is realized, and c_{it}(s^t) is i’s consumption under the history s^t.\n\n\n3.1.2 Optimization condition\nSolving this maximization problem, we can get a well-known result that, for any two households i and j in the village, the following holds: \n  \\frac{u'(c_{jt}(s^t))}{u'(c_{it}(s^t))} = \\frac{\\lambda_i}{\\lambda_j} \\quad \\forall s^t, \\forall t.\n\nIf I assume the utility function to take the CRRA form, that is, \n  u(c_{it}) = \\frac{c_{it}^{1 - \\sigma} - 1}{1 - \\sigma},\n then \n  u'(c_{it}) = c_{it}^{- \\sigma},\n and thus I get \n  \\frac{c_{jt}(s^t)^{-\\sigma}}{c_{it}(s^t)^{-\\sigma}} = \\frac{\\lambda_i}{\\lambda_j} \\quad \\forall s^t, \\forall t.\n Dropping s^t from the quation for simplicity and taking the logarithms, I obtain \n\\begin{aligned}\n  -\\sigma \\log(c_{jt}) + \\sigma \\log(c_{it}) &= \\log(\\lambda_i) - \\log(\\lambda_j) \\\\\n  \\Leftrightarrow \\log(c_{it}) &= \\frac{1}{\\sigma} \\left( \\log(\\lambda_i) - \\log(\\lambda_j) \\right) + \\log(c_{jt}) \\\\\n  \\Leftrightarrow N \\log(c_{it}) &= \\frac{1}{\\sigma} \\left( N \\log(\\lambda_i) - \\sum_j \\log(\\lambda_j) \\right) + \\sum_j \\log(c_{jt}) \\\\\n  \\Leftrightarrow \\log(c_{it}) &= \\frac{1}{\\sigma} \\left( \\log(\\lambda_i) - \\frac{1}{N} \\sum_j \\log(\\lambda_j) \\right) + \\frac{1}{N} \\sum_j \\log(c_{jt}).\n\\end{aligned}\n By defining the village-average consumption, c_{vt}, as \\log(c_{vt}) = \\frac{1}{N} \\sum_j \\log(c_{jt}), and taking the first difference from the equation above, I get \n  \\log(c_{it}) - \\log(c_{i, t - 1}) = \\log(c_{vt}) - \\log(c_{v, t - 1}).\n\n\n\n3.1.3 Introducing measurement errors\nHere I introduce the measurement error in consumption, which is assumed to be multiplicative and log-normally distributed. In particular, I denote observed consumption, c_{it}^* as c_{it}^* = c_{it} \\exp(\\varepsilon_{it}^c), where \\varepsilon_{it} \\sim N(0, \\gamma_c^2) is the measurement error which is i.i.d. across households and time. With this, the observed village-average consumption, c_{vt}^* is so that \n\\begin{aligned}\n  \\log(c_{vt}^*)\n  &= \\frac{1}{N} \\sum_i \\log(c_{it}^*) \\\\\n  &= \\frac{1}{N} \\sum_i \\log(c_{it}) + \\frac{1}{N} \\sum_i \\varepsilon_{it}^c \\\\\n  &= \\log(c_{vt}) + \\frac{1}{N} \\sum_i \\varepsilon_{it}^c \\\\\n  &= \\log(c_{vt}) + \\varepsilon_{vt}^c,\n\\end{aligned}\n where the village consumption measurement error is defined as \\varepsilon_{vt}^c \\equiv \\frac{1}{N} \\sum_i \\varepsilon_{it}^c. Substituting these into \\log(c_{it}) - \\log(c_{i, t - 1}) = \\log(c_{vt}) - \\log(c_{v, t - 1}), I obtain \n  \\log(c_{it}^*) - \\log(c_{i, t - 1}^*) = \\log(c_{vt}^*) - \\log(c_{v, t - 1}^*) + \\left(\\varepsilon_{it}^c - \\varepsilon_{vt}^c - (\\varepsilon_{i, t - 1}^c - \\varepsilon_{v, t - 1}^c) \\right).\n\n\n\n3.1.4 Likelihood function\nSince \\varepsilon_{it} \\sim N(0, \\gamma_C^2), I get \\varepsilon_{vt} \\sim N(0, \\frac{\\gamma_C^2}{N}), and note that Cov(\\varepsilon_{it}, \\varepsilon_{vt}) = \\frac{\\gamma_C^2}{N}. With the independence across periods and households of measurement errors, I obtain \n  Var(\\varepsilon_{it}^c - \\varepsilon_{vt}^c - (\\varepsilon_{i, t - 1}^c - \\varepsilon_{v, t - 1}^c) )\n  = 2\\left(\\gamma_C^2 + \\frac{\\gamma_C^2}{N} - 2\\frac{\\gamma_C^2}{N} \\right)\n  = 2 \\gamma_C^2 \\left(1 - \\frac{1}{N} \\right).\n\nTherefore, the likelihood function is\n\n  L(\\theta) = \\prod_{i} \\prod_{t = 2}^{T} f \\left( \\log \\left( \\frac{c_{it}^*}{c_{i, t - 1}^*} \\right) - \\log \\left( \\frac{c_{vt}^*}{c_{v, t - 1}^*} \\right), \\theta \\right) ,\n and the log likelihood function is \n  l(\\theta) = \\sum_{i} \\sum_{t = 2}^{T} \\log \\left(f \\left( \\log \\left( \\frac{c_{it}^*}{c_{i, t - 1}^*} \\right) - \\log \\left( \\frac{c_{vt}^*}{c_{v, t - 1}^*} \\right), \\theta\\right) \\right),\n where f(x, \\theta) is the density function of x \\sim N \\left(0, \\sqrt{ 2 \\gamma_C^2 \\left(1 - \\frac{1}{N} \\right)} \\right), and \\theta is a parameter to estimate (here, \\theta = \\gamma_C). In estimation, I find the parameter to maximize l(\\theta).\nWithout assuming that the model is correctly specified but assuming that there is no autocorrelation, the standard error of the parameter is A(\\widehat{\\theta})^{-1} B(\\widehat{\\theta}) A(\\widehat{\\theta})^{-1}, where A(\\widehat{\\theta}) is the Hessian matrix of the log-likelihood function and B(\\widehat{\\theta}) is the outer product of scores (= gradient of log likelihood).\n\n3.1.4.1 Notes\n\nThis model considers that any deviation from the full risk-sharing model is due to measurement errors in consumption.\nAs the paper points out, this model estimates parameters under the assumption that the model follows the full risk-sharing model. This is in contrast to many empirical papers which test a full riks-sharing model (such as Townsend (1994)).\nI derive the unconditional likelihood function, as opposed to the paper which used a conditional likelihood function with simulation. The author clarifies her choice to use the simulation method in a foortnote as follows: “Note that it is not necessary to use simulation to take into account measurement error in consumption at time t in the perfect risk-sharing case. I do it to be consistent with the LC case.”\nI do not rescale the “true” consumption (= consumption without measurement errors) as in the original code, since by the log scale, by construction, the means of true and observed consumption should be identical in expectation.\nInstead of \\log \\left( \\frac{1}{N} \\sum_i c_{it} \\right), I use \\frac{1}{N} \\log \\left( \\sum_i c_{it} \\right) for \\log(c_{vt}) since this is what the model above implies.\nI assume there is no auto correlation when calculating the standard errors since the measurement errors are assumed to be independent across periods."
  },
  {
    "objectID": "laczo2015_full_hom.html#rscript-for-estimation",
    "href": "laczo2015_full_hom.html#rscript-for-estimation",
    "title": "2  Full risk-sharing under homogeneous preferences",
    "section": "3.2 Rscript for estimation",
    "text": "3.2 Rscript for estimation\nThis R script estimates the full risk sharing model.\n\npacman::p_load(\n  tidyverse,\n  kableExtra,\n  numDeriv\n)\n\n\nload('IntermediateData/allData.RData')\n\n\ncalculateLogLikelihoodEachObsFullHom <- function(\n    param, \n    currentCons,\n    previousCons,\n    currentVilLogCons,\n    previousVilLogCons,\n    numHouseholds\n    ) {\n  \n  gammaC2 <- param\n  \n  logLikelihood <- dnorm(\n    (\n      log(currentCons / previousCons)\n      - (currentVilLogCons - previousVilLogCons)\n    ), sd = sqrt(2 * gammaC2 * (1 - 1 / numHouseholds)),\n    log = TRUE\n  )\n  return(logLikelihood)\n}\n\ncalculateScoreEachObsFullHom <- function(\n    param, \n    currentCons,\n    previousCons,\n    currentVilLogCons,\n    previousVilLogCons,\n    numHouseholds\n) {\n  grad(\n    calculateLogLikelihoodEachObsFullHom,\n    param,\n    currentCons = currentCons,\n    previousCons = previousCons,\n    currentVilLogCons = currentVilLogCons,\n    previousVilLogCons = previousVilLogCons,\n    numHouseholds = 34\n  )\n}\n\ncalculateScoreFullHom <- function(\n    param, \n    consdatByVillage,\n    vilMeanLogConsByVillage,\n    numHouseholds,\n    .tnum = tnum\n) {\n  \n  numParameters <- length(param)\n  \n  scoreMatrix <- matrix(0, nrow = numParameters, ncol = numParameters)\n  for (householdIndex in seq(1, numHouseholds)) {\n    for (periodIndex in seq(2, .tnum)) {\n      score <- calculateScoreEachObsFullHom(\n          param,\n          consdatByVillage[householdIndex, periodIndex],\n          consdatByVillage[householdIndex, (periodIndex - 1)],\n          vilMeanLogConsByVillage[periodIndex],\n          vilMeanLogConsByVillage[(periodIndex - 1)],\n          numHouseholds\n        )\n      scoreMatrix <- (\n        scoreMatrix\n        + outer(score, score)\n      )\n    }\n  }\n  return(scoreMatrix)\n}\n\ncalculateLogLikelihoodFullHom <- function(\n    param, \n    consdatByVillage,\n    vilMeanLogConsByVillage,\n    numHouseholds,\n    .tnum = tnum\n    ) {\n  \n  logLikelihood <- 0\n  for (householdIndex in seq(1, numHouseholds)) {\n    for (periodIndex in seq(2, .tnum)) {\n      logLikelihood <- (\n        logLikelihood\n        + calculateLogLikelihoodEachObsFullHom(\n          param,\n          consdatByVillage[householdIndex, periodIndex],\n          consdatByVillage[householdIndex, (periodIndex - 1)],\n          vilMeanLogConsByVillage[periodIndex],\n          vilMeanLogConsByVillage[(periodIndex - 1)],\n          numHouseholds\n        )\n      )\n    }\n  }\n    \n  return(- logLikelihood)\n}\n\ncalculateStandardErrors <- function(\n    estimationResult,\n    consdatByVillage,\n    vilMeanLogConsByVillage,\n    numHouseholds\n) {\n  (\n    solve(estimationResult$hessian) %*% \n    calculateScoreFullHom(\n      estimationResult$par,\n      consdatByVillage, \n      vilMeanLogConsByVillage,\n      numHouseholds\n    ) %*%\n    solve(estimationResult$hessian)\n  ) %>% sqrt %>% diag\n}\n\nestimateMLFullHom <- function(\n    village,\n    consdat,\n    vilMeanLogCons,\n    lowerBound,\n    upperBound,\n    .villageIndicatorMatrix = villageIndicatorMatrix\n) {\n  \n  numHouseholds <- .villageIndicatorMatrix[, village] %>% sum\n  consdatByVillage <- consdat[.villageIndicatorMatrix[, village], ]\n  vilMeanLogConsByVillage <- vilMeanLogCons[village, ]\n  \n  estimationResult <- optim(\n    0.1,\n    calculateLogLikelihoodFullHom,\n    consdatByVillage = consdatByVillage,\n    vilMeanLogConsByVillage = vilMeanLogConsByVillage,\n    numHouseholds = numHouseholds,\n    method = \"L-BFGS-B\",\n    lower = lowerBound,\n    upper = upperBound,\n    control = list(trace = 0, maxit = 200),\n    hessian = TRUE)\n  \n  standardErrors <- calculateStandardErrors(\n    estimationResult,\n    consdatByVillage,\n    vilMeanLogConsByVillage,\n    numHouseholds\n  )\n  \n  return(list(\n    parameter = estimationResult$par,\n    logLikelihood = - estimationResult$value,\n    standardErrors = standardErrors,\n    optimizationResult = estimationResult$message\n  ))\n}\n\nestimationResultFullHom <- map(\n  seq(1, numVillages),\n  ~ estimateMLFullHom(\n      .,\n      consdat,\n      vilMeanLogCons,\n      1e-3,\n      10\n  )\n)"
  },
  {
    "objectID": "laczo2015_dataprep.html#save-data",
    "href": "laczo2015_dataprep.html#save-data",
    "title": "1  Data preparation",
    "section": "1.6 Save data",
    "text": "1.6 Save data\n\nsave.image(\"IntermediateData/allData.RData\")"
  },
  {
    "objectID": "laczo2015_full_hom.html#code-for-estimation",
    "href": "laczo2015_full_hom.html#code-for-estimation",
    "title": "2  Full risk-sharing under homogeneous preferences",
    "section": "3.2 Code for estimation",
    "text": "3.2 Code for estimation\nThis R script estimates the full risk sharing model.\n\npacman::p_load(\n  tidyverse,\n  kableExtra,\n  numDeriv\n)\n\n\nload('IntermediateData/allData.RData')\n\n\ncalculateLogLikelihoodEachObsFullHom <- function(\n    param, \n    currentCons,\n    previousCons,\n    currentVilLogCons,\n    previousVilLogCons,\n    numHouseholds\n    ) {\n  \n  gammaC2 <- param\n  \n  logLikelihood <- dnorm(\n    (\n      log(currentCons / previousCons)\n      - (currentVilLogCons - previousVilLogCons)\n    ), sd = sqrt(2 * gammaC2 * (1 - 1 / numHouseholds)),\n    log = TRUE\n  )\n  return(logLikelihood)\n}\n\ncalculateScoreEachObsFullHom <- function(\n    param, \n    currentCons,\n    previousCons,\n    currentVilLogCons,\n    previousVilLogCons,\n    numHouseholds\n) {\n  grad(\n    calculateLogLikelihoodEachObsFullHom,\n    param,\n    currentCons = currentCons,\n    previousCons = previousCons,\n    currentVilLogCons = currentVilLogCons,\n    previousVilLogCons = previousVilLogCons,\n    numHouseholds = 34\n  )\n}\n\ncalculateScoreFullHom <- function(\n    param, \n    consdatByVillage,\n    vilMeanLogConsByVillage,\n    numHouseholds,\n    .tnum = tnum\n) {\n  \n  numParameters <- length(param)\n  \n  scoreMatrix <- matrix(0, nrow = numParameters, ncol = numParameters)\n  for (householdIndex in seq(1, numHouseholds)) {\n    for (periodIndex in seq(2, .tnum)) {\n      score <- calculateScoreEachObsFullHom(\n          param,\n          consdatByVillage[householdIndex, periodIndex],\n          consdatByVillage[householdIndex, (periodIndex - 1)],\n          vilMeanLogConsByVillage[periodIndex],\n          vilMeanLogConsByVillage[(periodIndex - 1)],\n          numHouseholds\n        )\n      scoreMatrix <- (\n        scoreMatrix\n        + outer(score, score)\n      )\n    }\n  }\n  return(scoreMatrix)\n}\n\ncalculateLogLikelihoodFullHom <- function(\n    param, \n    consdatByVillage,\n    vilMeanLogConsByVillage,\n    numHouseholds,\n    .tnum = tnum\n    ) {\n  \n  logLikelihood <- 0\n  for (householdIndex in seq(1, numHouseholds)) {\n    for (periodIndex in seq(2, .tnum)) {\n      logLikelihood <- (\n        logLikelihood\n        + calculateLogLikelihoodEachObsFullHom(\n          param,\n          consdatByVillage[householdIndex, periodIndex],\n          consdatByVillage[householdIndex, (periodIndex - 1)],\n          vilMeanLogConsByVillage[periodIndex],\n          vilMeanLogConsByVillage[(periodIndex - 1)],\n          numHouseholds\n        )\n      )\n    }\n  }\n    \n  return(- logLikelihood)\n}\n\ncalculateStandardErrors <- function(\n    estimationResult,\n    consdatByVillage,\n    vilMeanLogConsByVillage,\n    numHouseholds\n) {\n  (\n    solve(estimationResult$hessian) %*% \n    calculateScoreFullHom(\n      estimationResult$par,\n      consdatByVillage, \n      vilMeanLogConsByVillage,\n      numHouseholds\n    ) %*%\n    solve(estimationResult$hessian)\n  ) %>% sqrt %>% diag\n}\n\nestimateMLFullHom <- function(\n    village,\n    consdat,\n    vilMeanLogCons,\n    lowerBound,\n    upperBound,\n    .villageIndicatorMatrix = villageIndicatorMatrix\n) {\n  \n  numHouseholds <- .villageIndicatorMatrix[, village] %>% sum\n  consdatByVillage <- consdat[.villageIndicatorMatrix[, village], ]\n  vilMeanLogConsByVillage <- vilMeanLogCons[village, ]\n  \n  estimationResult <- optim(\n    0.1,\n    calculateLogLikelihoodFullHom,\n    consdatByVillage = consdatByVillage,\n    vilMeanLogConsByVillage = vilMeanLogConsByVillage,\n    numHouseholds = numHouseholds,\n    method = \"L-BFGS-B\",\n    lower = lowerBound,\n    upper = upperBound,\n    control = list(trace = 0, maxit = 200),\n    hessian = TRUE)\n  \n  standardErrors <- calculateStandardErrors(\n    estimationResult,\n    consdatByVillage,\n    vilMeanLogConsByVillage,\n    numHouseholds\n  )\n  \n  return(list(\n    parameter = estimationResult$par,\n    logLikelihood = - estimationResult$value,\n    standardErrors = standardErrors,\n    optimizationResult = estimationResult$message\n  ))\n}\n\nestimationResultFullHom <- map(\n  seq(1, numVillages),\n  ~ estimateMLFullHom(\n      .,\n      consdat,\n      vilMeanLogCons,\n      1e-3,\n      10\n  )\n)"
  },
  {
    "objectID": "laczo2015_lc_hom_model.html#model",
    "href": "laczo2015_lc_hom_model.html#model",
    "title": "3  Risk sharing with limited commitoment: model",
    "section": "3.1 Model",
    "text": "3.1 Model\nI consider constrained-efficient consumptioon allocatinos. The social planner solves the following problem:\n\\begin{align*}\n  &\\max_{\\{c_{it}(s^t)\\}} \\sum_i \\lambda_i \\sum_{t = 1}^{\\infty} \\sum_{s^t} \\delta^t \\pi(s^t) u(c_{it}(s^t)) \\\\\n  \\text{subject to}\n  &\\sum_i c_{it} (s^t) \\le \\sum_i y_{it}(s_t) \\quad \\forall s^t, \\forall t \\\\\n  &\\sum_{r = t}^{\\infty} \\sum_{s^r} \\delta^{r - t} \\pi(s^r | s^t) u(c_{ir}(s^r)) \\ge U_{i}^{aut}(s_t) \\quad \\forall s^t, \\forall t, \\forall i.\n\\end{align*}\nHere, the income follows a Markov process and is independent across households. Notice the difference between the history of states up to period t (s^t) and the state at period t (s_t). The variable \\lambda_i is the Pareto weight of a household i. The last equation is the participation constraints (PCs), whose RHS is the value of autarky and the solution of the following Bellman equation:\n\n  U_i^{aut}(s_t) = u((1 - \\phi) y_{it}(s_t)) + \\delta \\sum_{s^{t + 1}} \\pi(s_{t + 1} | s_t) U_{i}^{aut}(s_{t + 1}),\n where \\phi is the punishment of renege, which is a fraction of consumption each period. It is assumed that savings are absent.\nLetting the multiplier on the PC of i be \\delta^t \\pi(s^t) \\mu_i(s^t) and the multiplier on the aggregate resource constraint be \\delta^t \\pi(s^t) \\rho(s^t), the Lagrangian is\n\n  \\sum_{t = 1}^{\\infty} \\sum_{s^t} \\delta^t \\pi(s^t) \\left\\{ \\sum_i \\left[ \\lambda_i u_i(c_{it}(s^t)) + \\mu_i(s^t) \\left( \\sum_{r = t}^{\\infty} \\sum_{s^r} \\delta^{r - t} \\pi(s^r | s^t) u_i (c_{ir} (s^r)) - U_i^{aut}(s_t) \\right) \\right] + \\rho(s^t) \\left( \\sum_i \\left(y_{it} (s_t) - c_{it} (s^t) \\right) \\right) \\right\\}\n With the recursive method in Marcet and Marimon (2019), this Lagrangian can be written as\n\n  \\sum_{t = 1}^{\\infty} \\sum_{s^t} \\delta^t \\pi(s^t) \\left\\{ \\sum_i \\left[ M_i (s^{t - 1}) u_i (c_{it} (s^t)) + \\mu_i (s^t) (u_i (c_{it} (s^t)) - U_i^{aut} (s_t)) \\right] + \\rho(s^t) \\left( \\sum_i \\left( y_{it}(s_t) - c_{it} (s^t) \\right) \\right) \\right\\},\n\nwhere M_i(s^t) = M_i(s^{t - 1}) + \\mu_i(s^t) and M_i(s^0) = \\lambda. The variable M_i(s^t) is the current Pareto weight of household i and is equal to its initial Pareto weight plus the sum of the Lagrange mulipliers on its PCs along the history s^t.\nFrom the Lagrangian, the optimality condition is \n  u_i'(c_{it}(s^t)) M_i(s^t) = \\frac{\\rho(s^t)}{\\delta^t \\pi(s^t)},\n and thus, for two households i and j (i \\ne j), \n  u_i'(c_{it}(s^t)) M_i(s^t) = u_j'(c_{jt}(s^t)) M_j(s^t).\n Taking logarithms and summing over households j, I get \n\\begin{aligned}\n  \\log \\left(u'('c_{it}(s^t)) \\right) + \\log \\left(M_i(s^t) \\right) = \\frac{1}{N} \\sum_j \\log \\left(u(c_{jt}(s^t)) \\right) + \\frac{1}{N} \\sum_j \\log \\left(M_j(s^t) \\right).\n\\end{aligned}\n Defining the village consumption c_{vt} such that \\log \\left(u'(c_{vt}(s^t)) \\right) = \\frac{1}{N} \\sum_j \\log \\left(u(c_{jt}(s^t)) \\right) and normalizing the Pareto weight so that \\frac{1}{N} \\sum_j \\log \\left(M_j(s^t) \\right) = 0, I obtain \n\\begin{aligned}\n  \\log \\left(u'('c_{it}(s^t)) \\right) + \\log \\left(M_i(s^t) \\right) &= \\log \\left(u'(c_{vt}(s^t)) \\right) \\\\\n  \\Leftrightarrow \\frac{u'(c_{vt}(s^t))}{u'(c_{it}(s^t))} = M_i(s^t)\n\\end{aligned}\n Note that this is equivalent to the optimality condition that the ratio of marginal utilities between two “households”, i and v, equals the ratio of their Pareto weights, where the Pareto weight of v is normalized to 1. Therefore, when I consider the one-versus-rest risk-sharing, I can use this optimality condition as if the village is one household.\nLet x_i(s^t) = M_i(s^t), the “relative” Pareto weight of household i under the history s^t. Then, the vector of relative weights x(s^t) plays as a role as a co-state variable, and the solution consists of policy functions x_{it}(s_t, x_{t - 1}) and c_{it}(s_t, x_{t - 1}). That is, x_{t - 1} is a sufficient statistic for the history up to t - 1. The optimality condition is\n\n  \\frac{u_v'(c_{vt}(s_t, x_{t - 1}))}{u_i'(c_{it}(s_t, x_{t - 1}))} = x_{it}(s_t, x_{t - 1}) \\quad \\forall i.\n\nThe value functioon can be written recursively as\n\n  V_i(s_t, x_{t - 1}) = u_i (c_{it} (s_t, x_{t - 1})) + \\delta \\sum_{s_{t + 1}} \\pi(s_{t + 1} | s_t) V_i (s_{t + 1}, x_t(s_t, x_{t - 1})).\n\nThe evolution of relative Pareto weights is fully characterized by state-dependent intervals, which give the weights in the case where PCs are binding (Ligon, Thomas, and Worrall (2002))."
  },
  {
    "objectID": "laczo2015_lc_hom_model.html#code",
    "href": "laczo2015_lc_hom_model.html#code",
    "title": "3  Risk sharing with limited commitoment: model",
    "section": "3.2 Code",
    "text": "3.2 Code\n\npacman::p_load(\n  tidyverse,\n  kableExtra,\n  latex2exp\n)\n\n\n3.2.1 Load data\n\nload(\"IntermediateData/allData.RData\")\n\n\n\n3.2.2 Utility functions\n\ncalculateUtility <- function(cons, sigma) {\n  if (sigma != 1) {\n    utility = (cons^(1 - sigma) - 1) / (1 - sigma)\n  } else if (sigma == 1) {\n    utility = log(cons)\n  }\n  return(utility)\n}\ncalculateMarginalUtility <- function(cons, sigma) cons^(- sigma)\n\n\n\n3.2.3 Value of autarky\n\ncalculateAutarkyValue <- function(\n    incomeGridPoints, \n    sigma,\n    delta,\n    punishment,\n    incomeTransitionMatrix\n) {\n  \n  autarkyValue <- numeric(length = length(incomeGridPoints))\n  i <- 1\n  diff <- 1\n  while (diff > 1e-12) {\n    autarkyValueNew <- (\n      calculateUtility(incomeGridPoints * (1 - punishment), sigma) \n      + delta * incomeTransitionMatrix %*% autarkyValue\n    )\n    diff <- max(abs(autarkyValueNew - autarkyValue))\n    autarkyValue <- autarkyValueNew\n    i <- i + 1\n  }\n  return(autarkyValue)\n}\n\n\n\n3.2.4 Create grid of relative Pareto weights\nHere, I make the grid of relative Pareto weight of household 1, x(s^t), on which I compute the values (I use the notation x(s^t) rather than x_i(s^t) since there are only two households).\n\ngetRelativeParetoWeightsGridPoints <- function(\n    sigma,\n    punishment,\n    householdIncomeGridPoints,\n    villageIncomeGridPoints,\n    numRelativeParetoWeights\n    ) {\n  \n  minRelativeParetoWeights <- (\n    calculateMarginalUtility(max(villageIncomeGridPoints), sigma) \n    / calculateMarginalUtility(min(householdIncomeGridPoints * (1 - punishment)), sigma)\n  )\n  maxRelativeParetoWeights <- (\n    calculateMarginalUtility(min(villageIncomeGridPoints * (1 - punishment)), sigma) \n    / calculateMarginalUtility(max(householdIncomeGridPoints), sigma)\n  )\n  relativeParetoWeightsGridPoints <- exp(\n    seq(log(minRelativeParetoWeights), log(maxRelativeParetoWeights), length.out = numRelativeParetoWeights)\n    )\n  return(relativeParetoWeightsGridPoints)\n}\n\n\n\n3.2.5 Calculate consumption on the grid points\nThen, I compute consumptions of the household 1 on these grid points. From the optimality condition and the CRRA utility functions, we obtain\n\n  c_{1t} = \\frac{y_{1t} + y_{2t}}{1 + x_t^{- 1 / \\sigma}}.\n\n\ncalculateConsumption <- function(\n  aggregateIncome,\n  relativeParetoWeight,\n  sigma\n) {\n    aggregateIncome / (1 + relativeParetoWeight^(- 1 / sigma))\n}\n\n\n\n3.2.6 Caclulate values under full risk-sharing\nNow, I compute the values under full risk-sharing, which will be used as the initial values in value function iterations under the limited commitment model. Note that, under full risk sharing, the consumption only depends on the aggregate resources and time-invariate relative Pareto weights. Hence, I numerically solve the following Bellman equation:\n\n  V_i^{full}(s_t, x) = u_i(c_{it}(s_t, x)) + \\delta \\sum_{s^{t + 1}} \\pi(s_{t + 1} | s_t) V_{i}^{full}(s_{t + 1}, x).\n\n\ncalculateValueFullRiskSharing <- function(\n  incomeTransitionMatrix, \n  aggregateIncomeGridPoints, \n  delta, \n  sigma, \n  autarkyValueMatrix, \n  consumptionOnRelativeParetoWeightGrid,\n  numRelativeParetoWeights\n  ) {\n\n  # Initial guess is expected utilities under autarky\n  householdValueFullRiskSharing <- outer(\n    autarkyValueMatrix[, 1], rep(1, numRelativeParetoWeights)\n    )\n  villageValueFullRiskSharing <- outer(\n    autarkyValueMatrix[, 2], rep(1, numRelativeParetoWeights)\n    )\n\n  iteration <- 1\n  diff <- 1\n  while (diff > 1e-10 & iteration < 500) {\n    householdValueFullRiskSharingNew <- (\n      calculateUtility(consumptionOnRelativeParetoWeightGrid, sigma) \n      + delta * incomeTransitionMatrix %*% householdValueFullRiskSharing\n    )\n    villageValueFullRiskSharingNew <- (\n      calculateUtility(aggregateIncomeGridPoints - consumptionOnRelativeParetoWeightGrid, sigma) \n      + delta * incomeTransitionMatrix %*% villageValueFullRiskSharing\n    )\n    diff <- max(\n      max(abs(householdValueFullRiskSharing - householdValueFullRiskSharingNew)), \n      max(abs(villageValueFullRiskSharing - villageValueFullRiskSharingNew))\n      )\n    householdValueFullRiskSharing <- householdValueFullRiskSharingNew\n    villageValueFullRiskSharing <- villageValueFullRiskSharingNew\n    iteration <- iteration + 1\n  }\n\n  return(list(\n    householdValueFullRiskSharing = householdValueFullRiskSharing, \n    villageValueFullRiskSharing = villageValueFullRiskSharing\n    ))\n}\n\n\n\n3.2.7 Values under risk-sharing (limited commitment)\nNext, I derive the state-dependent intervals of relative Pareto weights and calculate values under the model of limited commitment. To derive the intervals, I use the fact that at the limits of the intervals, the PCs are binding. For instance, to compute the lower limit \\underline{x}^h(s), where h indicates h’th iteration, the PC of the household 1 is binding:\n\n  u_1(c_1^h(s)) + \\delta \\sum_{s'} \\pi(s' | s) V_1^{h - i} (s', \\underline{x}^h(s)) = U_1^{aut}(s),\n where the optimality condition is\n\n  \\frac{u_2'(c_{2}^h(s))}{u_1'(c_{1}^h(s))} = \\underline{x}^h(s).\n\nNotice that, once the PC binds, the past history, which is summarized by x_{t - 1}, does not matter. This property is called “amnesia” (Kocherlakota (1996)) or “forgiveness” (Ligon, Thomas, and Worrall (2002)). Since \\underline{x}^h(s) may not be on the grid q, linear interpolation is used to compute V_1^{h - 1}(s', \\underline{x}^h(s)).\nSimilarly, \\overline{x}^h(s) is computed using the binding PC of the household 2\n\n  u_2(c_2^h(s)) + \\delta \\sum_{s'} \\pi(s' | s) V_2^{h - i} (s', \\overline{x}^h(s)) = U_2^{aut}(s),\n where the optimality condition is\n\n  \\frac{u_2'(c_{2}^h(s))}{u_1'(c_{1}^h(s))} = \\overline{x}^h(s).\n\nAfter deriving these limits of intervals,\n\nfor relative Pareto weights below \\underline{x}^h(s), compute consumption of HH1 based on \\underline{x}^h(s) and let its value be U_1^{aut};\nfor relative Pareto weights above \\overline{x}^h(s), compute consumption of HH1 based on \\overline{x}^h(s) and let the value of HH2 be U_2^{aut};\nfor other relative Pareto weights, use them to compute consumption of HH1 and the values of households.\n\nBy iterating these steps, we can calculate the value functions of households and limits of intervals.\n\ninterpolateValueFunction <- function(\n    relativeParetoWeight,\n    relativeParetoWeightsGridPoints,\n    valueFunctionMatrix\n    ) {\n  apply(\n    valueFunctionMatrix,\n    1,\n    function(y) {\n      approxfun(\n        relativeParetoWeightsGridPoints, \n        y, \n        rule = 2\n        )(relativeParetoWeight)\n    }\n    )\n}\n\ncalculateDiffLCRiskSharingAndAutarky <- function(\n    relativeParetoWeight,\n    relativeParetoWeightsGridPoints,\n    delta,\n    sigma,\n    aggregateIncome,\n    householdValueLCRiskSharing,\n    villageValueLCRiskSharing,\n    incomeTransitionProbVec,\n    householdAutarkyValue,\n    villageAutarkyValue\n    ) {\n  \n  householdConsumption <- calculateConsumption(\n    aggregateIncome,\n    relativeParetoWeight,\n    sigma\n  )\n  \n  householdValueLCRiskSharingAtRelativeParetoWeight <- interpolateValueFunction(\n    relativeParetoWeight,\n    relativeParetoWeightsGridPoints,\n    householdValueLCRiskSharing\n    )\n  villageValueLCRiskSharingAtRelativeParetoWeight <- interpolateValueFunction(\n    relativeParetoWeight,\n    relativeParetoWeightsGridPoints,\n    villageValueLCRiskSharing\n    )\n  \n  householdDiffLCRiskSharingAndAutarky <- (\n    calculateUtility(householdConsumption, sigma) \n    + delta * incomeTransitionProbVec %*% householdValueLCRiskSharingAtRelativeParetoWeight \n    - householdAutarkyValue\n  ) %>% as.numeric\n  villageDiffLCRiskSharingAndAutarky <- (\n    calculateUtility(aggregateIncome - householdConsumption, sigma) \n    + delta * incomeTransitionProbVec %*% villageValueLCRiskSharingAtRelativeParetoWeight \n    - villageAutarkyValue\n  ) %>% as.numeric\n\n  return(list(\n    householdDiffLCRiskSharingAndAutarky = householdDiffLCRiskSharingAndAutarky,\n    villageDiffLCRiskSharingAndAutarky = villageDiffLCRiskSharingAndAutarky\n  ))\n}\n\ngetClosestGridIndex <- function(\n  point,\n  gridPoints\n) {\n  closestGridIndex <- which.min(\n    abs(point - gridPoints) \n  )\n  if (\n    point \n    > gridPoints[closestGridIndex]\n    ) {\n    closestGridIndex <- closestGridIndex + 1\n  }\n  return(closestGridIndex)\n}\n\ncalculateValueLCRiskSharing <- function(\n  valueFullRiskSharing,\n  consumptionOnRelativeParetoWeightGrid,\n  aggregateIncomeGridPoints,\n  incomeTransitionMatrix,\n  autarkyValueMatrix,\n  relativeParetoWeightsGridPoints,\n  numRelativeParetoWeights,\n  delta,\n  sigma,\n  numIncomeStates,\n  iterationLimit,\n  diffLimit\n) {\n  \n  # Initial guess is expected utilities under full risk sharing\n  householdValueLCRiskSharing <- valueFullRiskSharing$householdValueFullRiskSharing\n  villageValueLCRiskSharing <- valueFullRiskSharing$villageValueFullRiskSharing\n  \n  relativeParetoWeightBounds <- matrix(NA, nrow = numIncomeStates, ncol = 2)\n  \n  diff <- 1\n  iteration <- 1\n  while ((diff > diffLimit) && (iteration <= iterationLimit)) {\n    \n    # First, ignore enforceability and just update the value functions\n    # using the values at the previous iteration\n    householdValueLCRiskSharingNew <- (\n      calculateUtility(consumptionOnRelativeParetoWeightGrid, sigma) \n      + delta * incomeTransitionMatrix %*% householdValueLCRiskSharing\n    )\n    villageValueLCRiskSharingNew <- (\n      calculateUtility(aggregateIncomeGridPoints - consumptionOnRelativeParetoWeightGrid, sigma) \n      + delta * incomeTransitionMatrix %*% villageValueLCRiskSharing\n    )\n    \n    # Now check enforceability at each state\n    for (incomeStateIndex in seq(1, numIncomeStates)) {\n      aggregateIncome <- aggregateIncomeGridPoints[incomeStateIndex]\n      incomeTransitionProbVec <- incomeTransitionMatrix[incomeStateIndex,]\n      householdAutarkyValue <- autarkyValueMatrix[incomeStateIndex, 1]\n      villageAutarkyValue <- autarkyValueMatrix[incomeStateIndex, 2]\n      \n      diffLCRiskSharingAndAutarky <- map(\n        c(min(relativeParetoWeightsGridPoints), max(relativeParetoWeightsGridPoints)),\n        ~ calculateDiffLCRiskSharingAndAutarky(\n          .,\n          relativeParetoWeightsGridPoints,\n          delta,\n          sigma,\n          aggregateIncome,\n          householdValueLCRiskSharing,\n          villageValueLCRiskSharing,\n          incomeTransitionProbVec,\n          householdAutarkyValue,\n          villageAutarkyValue\n          )   \n      ) %>% \n        setNames(c(\"minRelativeParetoWeight\", \"maxRelativeParetoWeight\"))\n      \n      # If the relative Pareto weight is too low and violates the PC, then\n      # set the relative Pareto weight to the lower bound of the interval, and\n      # HH gets the value under autarky.\n      if (\n        (diffLCRiskSharingAndAutarky %>% \n         .$maxRelativeParetoWeight %>% \n         .$householdDiffLCRiskSharingAndAutarky > 0) \n        && (diffLCRiskSharingAndAutarky %>% \n            .$minRelativeParetoWeight %>% \n            .$householdDiffLCRiskSharingAndAutarky < 0)) {\n        \n        relativeParetoWeightLowerBound <- uniroot(\n          function(x) {calculateDiffLCRiskSharingAndAutarky(\n            x,\n            relativeParetoWeightsGridPoints,\n            delta,\n            sigma,\n            aggregateIncome,\n            householdValueLCRiskSharing,\n            villageValueLCRiskSharing,\n            incomeTransitionProbVec,\n            householdAutarkyValue,\n            villageAutarkyValue\n            )$householdDiffLCRiskSharingAndAutarky}, \n          c(min(relativeParetoWeightsGridPoints), max(relativeParetoWeightsGridPoints)), \n          tol = 1e-10, \n          maxiter = 300\n          )$root\n        relativeParetoWeightBounds[incomeStateIndex, 1] <- relativeParetoWeightLowerBound\n\n        relativeParetoWeightLowerBoundIndex <- getClosestGridIndex(\n          relativeParetoWeightLowerBound,\n          relativeParetoWeightsGridPoints\n        )\n        \n        villageValueLCRiskSharingLowerBound <- interpolateValueFunction(\n          relativeParetoWeightLowerBound,\n          relativeParetoWeightsGridPoints,\n          villageValueLCRiskSharing\n        )\n        \n        householdConsumptionLowerBound <- calculateConsumption(\n          aggregateIncome,\n          relativeParetoWeightLowerBound,\n          sigma\n        )       \n        \n        householdValueLCRiskSharingNew[\n          incomeStateIndex, \n          seq(1, relativeParetoWeightLowerBoundIndex)\n          ] <- householdAutarkyValue\n        villageValueLCRiskSharingNew[\n          incomeStateIndex, \n          seq(1, relativeParetoWeightLowerBoundIndex)\n          ] <- (\n          calculateUtility(aggregateIncome - householdConsumptionLowerBound, sigma)\n          + delta * incomeTransitionProbVec %*% villageValueLCRiskSharingLowerBound\n        )\n      } else if (\n        diffLCRiskSharingAndAutarky %>% \n        .$maxRelativeParetoWeight %>% \n        .$householdDiffLCRiskSharingAndAutarky <= 0\n        ) {\n        householdValueLCRiskSharingNew[incomeStateIndex,] <- householdAutarkyValue\n        villageValueLCRiskSharingNew[incomeStateIndex,] <- villageAutarkyValue\n        relativeParetoWeightBounds[incomeStateIndex, 1] <- max(relativeParetoWeightsGridPoints)\n        relativeParetoWeightLowerBoundIndex <- numRelativeParetoWeights + 1\n      } else if (\n        diffLCRiskSharingAndAutarky %>% \n        .$minRelativeParetoWeight %>% \n        .$householdDiffLCRiskSharingAndAutarky >= 0\n        ) {\n        relativeParetoWeightBounds[incomeStateIndex, 1] <- min(relativeParetoWeightsGridPoints)\n        relativeParetoWeightLowerBoundIndex <- 0\n      }\n    \n      # If the relative Pareto weight is too high and violates the PC, then\n      # set the relative Pareto weight to the upper bound of the interval, and\n      # village gets the value under autarky.\n      if (relativeParetoWeightLowerBoundIndex <= numRelativeParetoWeights) {\n        if (\n          (diffLCRiskSharingAndAutarky %>% \n           .$minRelativeParetoWeight %>% \n           .$villageDiffLCRiskSharingAndAutarky > 0)\n          && (diffLCRiskSharingAndAutarky %>% \n              .$maxRelativeParetoWeight %>% \n              .$villageDiffLCRiskSharingAndAutarky < 0)\n          ) {\n          \n          relativeParetoWeightUpperBound <- uniroot(\n            function(x) {calculateDiffLCRiskSharingAndAutarky(\n              x,\n              relativeParetoWeightsGridPoints,\n              delta,\n              sigma,\n              aggregateIncome,\n              householdValueLCRiskSharing,\n              villageValueLCRiskSharing,\n              incomeTransitionProbVec,\n              householdAutarkyValue,\n              villageAutarkyValue\n              )$villageDiffLCRiskSharingAndAutarky}, \n            c(min(relativeParetoWeightsGridPoints), max(relativeParetoWeightsGridPoints)), \n            tol = 1e-10, \n            maxiter = 300\n            )$root\n          relativeParetoWeightBounds[incomeStateIndex, 2] <- relativeParetoWeightUpperBound\n          \n          relativeParetoWeightUpperBoundIndex <- getClosestGridIndex(\n            relativeParetoWeightUpperBound,\n            relativeParetoWeightsGridPoints\n          )\n           \n          householdValueLCRiskSharingUpperBound <- interpolateValueFunction(\n            relativeParetoWeightUpperBound,\n            relativeParetoWeightsGridPoints,\n            householdValueLCRiskSharing\n          )\n          \n          householdConsumptionUpperBound <- calculateConsumption(\n            aggregateIncome,\n            relativeParetoWeightUpperBound,\n            sigma\n          )       \n          \n          householdValueLCRiskSharingNew[\n            incomeStateIndex, \n            seq(relativeParetoWeightUpperBoundIndex, numRelativeParetoWeights)\n            ] <- (\n            calculateUtility(householdConsumptionUpperBound, sigma)\n            + delta * incomeTransitionProbVec %*% householdValueLCRiskSharingUpperBound\n            )\n          villageValueLCRiskSharingNew[\n            incomeStateIndex, \n            seq(relativeParetoWeightUpperBoundIndex, numRelativeParetoWeights)\n            ] <- villageAutarkyValue\n        } else if (\n          (diffLCRiskSharingAndAutarky %>% \n           .$minRelativeParetoWeight %>% \n           .$villageDiffLCRiskSharingAndAutarky <= 0)\n          ) {\n          householdValueLCRiskSharingNew[incomeStateIndex,] <- householdAutarkyValue\n          villageValueLCRiskSharingNew[incomeStateIndex,] <- villageAutarkyValue\n          relativeParetoWeightBounds[incomeStateIndex, 2] <- min(relativeParetoWeightsGridPoints)\n          relativeParetoWeightUpperBoundIndex <- 0\n        } else if (\n          (diffLCRiskSharingAndAutarky %>% \n           .$maxRelativeParetoWeight %>% \n           .$villageDiffLCRiskSharingAndAutarky >= 0)\n          ) {\n          relativeParetoWeightBounds[incomeStateIndex, 2] <- max(relativeParetoWeightsGridPoints)\n          relativeParetoWeightUpperBoundIndex <- numRelativeParetoWeights + 1\n        }\n      }\n      \n      # The case where the relative Pareto weight does not violate PC\n      if ((relativeParetoWeightLowerBoundIndex + 1) < (relativeParetoWeightUpperBoundIndex - 1)) {\n        householdValueLCRiskSharingNew[\n          incomeStateIndex, \n          (relativeParetoWeightLowerBoundIndex + 1):(relativeParetoWeightUpperBoundIndex - 1)\n          ] <- (\n            calculateUtility(\n              consumptionOnRelativeParetoWeightGrid[\n                incomeStateIndex, \n                (relativeParetoWeightLowerBoundIndex + 1):(relativeParetoWeightUpperBoundIndex - 1)\n                ], \n              sigma\n              ) \n            + delta * incomeTransitionProbVec %*% householdValueLCRiskSharing[\n              , (relativeParetoWeightLowerBoundIndex + 1):(relativeParetoWeightUpperBoundIndex - 1)\n            ]\n          )\n        villageValueLCRiskSharingNew[\n          incomeStateIndex, \n          (relativeParetoWeightLowerBoundIndex + 1):(relativeParetoWeightUpperBoundIndex - 1)\n          ] <- (\n            calculateUtility(\n              aggregateIncome - consumptionOnRelativeParetoWeightGrid[\n                incomeStateIndex, \n                (relativeParetoWeightLowerBoundIndex + 1):(relativeParetoWeightUpperBoundIndex - 1)\n                ], \n              sigma) \n            + delta * incomeTransitionProbVec %*% villageValueLCRiskSharing[\n              , (relativeParetoWeightLowerBoundIndex + 1):(relativeParetoWeightUpperBoundIndex - 1)\n              ] \n            )\n        }\n      }   \n\n    diff <- max(\n      max(abs(householdValueLCRiskSharingNew - householdValueLCRiskSharing)),\n      max(abs(villageValueLCRiskSharingNew - villageValueLCRiskSharing))\n    )\n    householdValueLCRiskSharing <- householdValueLCRiskSharingNew\n    villageValueLCRiskSharing <- villageValueLCRiskSharingNew\n    iteration <- iteration + 1\n  }\n  \n  print(iteration)\n\n  if (iteration == iterationLimit) {\n    print(\"Reached the maximum limit of iterations!\")\n  }\n  \n  return(list(\n    householdValueLCRiskSharing = householdValueLCRiskSharing,\n    villageValueLCRiskSharing = villageValueLCRiskSharing,\n    relativeParetoWeightBounds = relativeParetoWeightBounds))\n}\n\n\nsolveLCRiskSharingByVillage <- function(\n    delta,\n    sigma,\n    punishment,\n    householdIncomeTransitionMatrix,\n    householdIncomeGridPoints,\n    villageIncomeTransitionMatrix,\n    villageIncomeGridPoints,\n    numRelativeParetoWeights = 200,\n    iterationLimit = 100,\n    diffLimit = 1e-8\n){\n  \n  incomeTransitionMatrix <- kronecker(\n    villageIncomeTransitionMatrix,\n    householdIncomeTransitionMatrix\n    )\n  \n  numIncomeStates <- length(householdIncomeGridPoints) * length(villageIncomeGridPoints)\n  \n  incomeGridPointsMatrix <- as.matrix(expand.grid(\n    householdIncomeGridPoints, villageIncomeGridPoints\n    ))\n  \n  aggregateIncomeGridPoints <- rowSums(incomeGridPointsMatrix)\n  \n  autarkyValueMatrix <- expand.grid(\n    calculateAutarkyValue(\n      householdIncomeGridPoints,\n      sigma,\n      delta,\n      punishment,\n      householdIncomeTransitionMatrix\n    ),\n    calculateAutarkyValue(\n      villageIncomeGridPoints,\n      sigma,\n      delta,\n      punishment,\n      villageIncomeTransitionMatrix\n    )\n  )\n  \n  relativeParetoWeightsGridPoints <- getRelativeParetoWeightsGridPoints(\n      sigma,\n      punishment,\n      householdIncomeGridPoints,\n      villageIncomeGridPoints,\n      numRelativeParetoWeights\n      )\n  \n  consumptionOnRelativeParetoWeightGrid <- matrix(\n    NA, nrow = numIncomeStates, ncol = numRelativeParetoWeights\n    )\n  for (incomeStateIndex in seq_along(aggregateIncomeGridPoints)) {\n    for (relativeParetoWeightIndex in seq_along(relativeParetoWeightsGridPoints)) {\n      consumptionOnRelativeParetoWeightGrid[\n        incomeStateIndex, \n        relativeParetoWeightIndex\n        ] <- calculateConsumption(\n          aggregateIncomeGridPoints[incomeStateIndex],\n          relativeParetoWeightsGridPoints[relativeParetoWeightIndex],\n          sigma\n        )\n      }\n    }\n\n  valueFullRiskSharing <- calculateValueFullRiskSharing(\n    incomeTransitionMatrix, \n    aggregateIncomeGridPoints, \n    delta, \n    sigma, \n    autarkyValueMatrix, \n    consumptionOnRelativeParetoWeightGrid,\n    numRelativeParetoWeights\n    )\n\n  valueLCRiskSharing <- calculateValueLCRiskSharing(\n    valueFullRiskSharing,\n    consumptionOnRelativeParetoWeightGrid,\n    aggregateIncomeGridPoints,\n    incomeTransitionMatrix,\n    autarkyValueMatrix,\n    relativeParetoWeightsGridPoints,\n    numRelativeParetoWeights,\n    delta,\n    sigma,\n    numIncomeStates,\n    iterationLimit,\n    diffLimit\n  )\n\n  return(valueLCRiskSharing)\n}\n\n\n\n3.2.8 Sanity test: replication of Figure 1 in Ligon, Thomas, and Worrall (2002)\nFor the sanity test of this function, I use it to replicate the Figure 1 in Ligon, Thomas, and Worrall (2002). Here I use the parameter values in the original paper. I choose the income process (y_l, y_h) = (2/3, 4/3) and (p_l, p_h) = (0.1, 0.9) for both households so that the mean is 1 and the ratio y_l / y_h is 1/2 as in the paper. Also, the penalty under autarky is absent as in the original numerical exercise. Finally, I assume the CRRA utility functions:\n\n  u(c_{it}) = \\frac{c_{it}^{1 - \\sigma} - 1}{1 - \\sigma}.\n\n\nsigmaLTW <- 1.0\npunishmentLTW <- 0.0\n\nincomeTransitionMatrixLTW <- matrix(rep(c(0.1, 0.9), 2), nrow = 2, byrow = TRUE)\nincomeGridPointsLTW <- c(2/3, 4/3)\nnumIncomeStates <- length(incomeGridPointsLTW) *  length(incomeGridPointsLTW)\n\ndeltaVec <- seq(0.8, 0.999, by = 0.002)\n\n\nsolveLCRiskSharingByVillage(\n  0.9,\n  sigmaLTW,\n  punishmentLTW,\n  incomeTransitionMatrixLTW,\n  incomeGridPointsLTW,\n  incomeTransitionMatrixLTW,\n  incomeGridPointsLTW,\n  numRelativeParetoWeights = 400,\n  iterationLimit = 1000,\n  diffLimit = 1e-8\n  )\n\nLCRiskSharingResultLTW <- map(\n  deltaVec,\n  ~ solveLCRiskSharingByVillage(\n    .,\n    sigmaLTW,\n    punishmentLTW,\n    incomeTransitionMatrixLTW,\n    incomeGridPointsLTW,\n    incomeTransitionMatrixLTW,\n    incomeGridPointsLTW,\n    numRelativeParetoWeights = 400,\n    iterationLimit = 1000,\n    diffLimit = 1e-8\n    )\n)\n\n\n\n\n\n\n\n\nrelativeParetoWeightBoundsArray = array(\n  NA, \n  dim = c(numIncomeStates, 2, length(deltaVec))\n  )\n\nfor (deltaIndex in seq_along(deltaVec)) {\n  relativeParetoWeightBoundsArray[,,deltaIndex] <- (\n    LCRiskSharingResultLTW[[deltaIndex]]$relativeParetoWeightBounds\n  )\n}\n\n\nggplot() +\n  geom_line(aes(deltaVec, log(relativeParetoWeightBoundsArray[1,1,]), color = \"a\")) +\n  geom_line(aes(deltaVec, log(relativeParetoWeightBoundsArray[1,2,]), color = \"b\")) +\n  geom_line(aes(deltaVec, log(relativeParetoWeightBoundsArray[2,1,]), color = \"c\")) +\n  geom_line(aes(deltaVec, log(relativeParetoWeightBoundsArray[2,2,]), color = \"d\")) +\n  geom_line(aes(deltaVec, log(relativeParetoWeightBoundsArray[3,1,]), color = \"e\")) +\n  geom_line(aes(deltaVec, log(relativeParetoWeightBoundsArray[3,2,]), color = \"f\")) +\n  geom_line(aes(deltaVec, log(relativeParetoWeightBoundsArray[4,1,]), color = \"g\")) +\n  geom_line(aes(deltaVec, log(relativeParetoWeightBoundsArray[4,2,]), color = \"h\")) +\n  coord_cartesian(\n    xlim = c(0.8, 1.0), \n    ylim = c(\n      log(incomeGridPointsLTW[1] / incomeGridPointsLTW[2]),\n      log(incomeGridPointsLTW[2] / incomeGridPointsLTW[1])\n      )\n    ) +\n  geom_ribbon(aes(x = deltaVec,\n                  ymin = log(relativeParetoWeightBoundsArray[1,1,]),\n                  ymax = log(relativeParetoWeightBoundsArray[1,2,])),\n                  fill = \"blue\", alpha = 0.2) +\n  geom_ribbon(aes(x = deltaVec,\n                  ymin = log(relativeParetoWeightBoundsArray[2,1,]),\n                  ymax = log(relativeParetoWeightBoundsArray[2,2,])),\n                  fill = \"red\", alpha = 0.2) +\n  geom_ribbon(aes(x = deltaVec,\n                  ymin = log(relativeParetoWeightBoundsArray[3,1,]),\n                  ymax = log(relativeParetoWeightBoundsArray[3,2,])),\n                  fill = \"green\", alpha = 0.2) +\n  geom_ribbon(aes(x = deltaVec,\n                  ymin = log(relativeParetoWeightBoundsArray[4,1,]),\n                  ymax = log(relativeParetoWeightBoundsArray[4,2,])),\n                  fill = \"yellow\", alpha = 0.2) +\n  scale_color_manual(\n    name = \"End-points\",\n    values = c(\n      \"blue\",\n      \"purple\",\n      \"brown\",\n      \"red\",\n      \"yellow\",\n      \"green\",\n      \"orange\",\n      \"gray\"\n      ),\n    labels = unname(TeX(c(\n      \"$\\\\underline{x}_{ll}$\",\n      \"$\\\\bar{x}_{ll}$\",\n      \"$\\\\underline{x}_{hl}$\",\n      \"$\\\\bar{x}_{hl}$\",\n      \"$\\\\underline{x}_{lh}$\",\n      \"$\\\\bar{x}_{lh}$\",\n      \"$\\\\underline{x}_{hh}$\",\n      \"$\\\\bar{x}_{hh}$\"\n      )))\n    ) +\n  xlab(\"Discount factor (delta)\") +\n  ylab(\"log of the relative Pareto weights (x)\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nKocherlakota, N. R. 1996. “Implications of Efficient Risk Sharing without Commitment.” The Review of Economic Studies 63 (4): 595–609. https://doi.org/10.2307/2297795.\n\n\nLigon, Ethan, Jonathan P. Thomas, and Tim Worrall. 2002. “Informal Insurance Arrangements with Limited Commitment: Theory and Evidence from Village Economies.” Review of Economic Studies 69 (1): 209–44. https://doi.org/10.1111/1467-937X.00204.\n\n\nMarcet, Albert, and Ramon Marimon. 2019. “Recursive Contracts.” Econometrica 87 (5): 1589–1631."
  },
  {
    "objectID": "laczo2015_full_hom.html#estimation-result",
    "href": "laczo2015_full_hom.html#estimation-result",
    "title": "2  Full risk-sharing under homogeneous preferences",
    "section": "3.3 Estimation result",
    "text": "3.3 Estimation result\n\nestimationFullHomTable <- do.call(\n  cbind,\n  map(\n    seq(1, numVillages),\n    ~ c(\n      formatC(estimationResultFullHom[[.]]$parameter, digits = 3, format = \"f\"),\n      str_interp('(${formatC(estimationResultFullHom[[.]]$standardErrors, digits = 3, format = \"f\")})'),\n      formatC(estimationResultFullHom[[.]]$logLikelihood, digits = 3, format = \"f\")\n    )\n  )\n)\ncolnames(estimationFullHomTable) <- c(\"Aurepalle\", \"Kanzara\", \"Shirapur\")\nrownames(estimationFullHomTable) <- c(\n  \"Variance of consumption measurement errors\",\n  \"\",\n  \"Log likelihood\"\n  )\n\nestimationFullHomTable %>% \n  kbl(digits = 3) %>% \n  kable_classic()\n\n\n\n \n  \n      \n    Aurepalle \n    Kanzara \n    Shirapur \n  \n \n\n  \n    Variance of consumption measurement errors \n    0.036 \n    0.037 \n    0.048 \n  \n  \n     \n    (0.004) \n    (0.010) \n    (0.007) \n  \n  \n    Log likelihood \n    -13.808 \n    -20.166 \n    -36.422"
  }
]